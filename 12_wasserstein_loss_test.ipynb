{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "from wasserstein_loss import *\n",
    "from train_on_NDI import *\n",
    "import torchvision\n",
    "import time\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn((32, 1, 200, 200))\n",
    "model = torchvision.models.resnet50()\n",
    "model.conv1 = nn.Conv2d(1, model.conv1.out_channels, model.conv1.kernel_size, model.conv1.stride, \n",
    "                        model.conv1.padding, model.conv1.dilation, model.conv1.groups)\n",
    "model.fc = nn.Linear(model.fc.in_features, 512)\n",
    "outputs = []\n",
    "x1 = model.maxpool(model.relu(model.bn1(model.conv1(inputs))))\n",
    "outputs.append(x1)\n",
    "x2 = model.layer1(x1)\n",
    "outputs.append(x2)\n",
    "x3 = model.layer2(x2)\n",
    "outputs.append(x3)\n",
    "x4 = model.layer3(x3)\n",
    "outputs.append(x4)\n",
    "x5 = model.layer4(x4)\n",
    "outputs.append(x5)\n",
    "x6 = model.fc(model.avgpool(x5).flatten(1))\n",
    "outputs.append(x6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 50, 50])\n",
      "torch.Size([32, 256, 50, 50])\n",
      "torch.Size([32, 512, 25, 25])\n",
      "torch.Size([32, 1024, 13, 13])\n",
      "torch.Size([32, 2048, 7, 7])\n",
      "torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "for x in outputs:\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.pooling.AdaptiveAvgPool2d"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.avgpool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformNet(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(TransformNet, self).__init__()\n",
    "        self.size = size\n",
    "        self.net = nn.Sequential(nn.Linear(self.size, self.size))\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.net(input)\n",
    "        return out / torch.sqrt(torch.sum(out ** 2, dim=1, keepdim=True))\n",
    "\n",
    "def get_self_pretrain_model(index=1000):\n",
    "    base_encoder = torchvision.models.resnet50(weights=None)\n",
    "    base_encoder.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    origin_dim_mlp = base_encoder.fc.in_features\n",
    "    base_encoder.fc = None\n",
    "    temp = torch.load(f'./checkpoints/ImageNet_ALL_CHECK_{index}_Epoch.pth')['state_dict']\n",
    "    state_dict = {}\n",
    "    for k, v in temp.items():\n",
    "        if 'encoder_q' in k:\n",
    "            if 'fc' not in k:\n",
    "                state_dict['.'.join(k.split('.')[1:])] = v\n",
    "    base_encoder.load_state_dict(state_dict)\n",
    "    base_encoder.fc = torch.nn.Linear(origin_dim_mlp, 512)\n",
    "    return base_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_NN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(512, 512)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 10\n",
    "lam = 1\n",
    "pro = rand_projections(512, 1024).to('cuda')\n",
    "inputs_1, inputs_2 = torch.randn((32, 512)).cuda(), torch.randn((32, 512)).cuda()\n",
    "simple_net = simple_NN()\n",
    "simple_net.cuda()\n",
    "first_samples = simple_net(inputs_1)\n",
    "second_samples = simple_net(inputs_2)\n",
    "first_samples_detach = first_samples.detach()\n",
    "second_samples_detach = second_samples.detach()\n",
    "f = TransformNet(512).to('cuda')\n",
    "f_op = torch.optim.Adam(f.parameters(), lr=0.0005, betas=(0.5, 0.999))\n",
    "p = 2\n",
    "\n",
    "for _ in range(max_iter):\n",
    "    projections = f(pro)\n",
    "    cos = cosine_distance_torch(projections, projections)\n",
    "    reg = lam * cos\n",
    "    encoded_projections = first_samples_detach.matmul(projections.transpose(0, 1))\n",
    "    distribution_projections = second_samples_detach.matmul(projections.transpose(0, 1))\n",
    "    wasserstein_distance = get_wasserstein_distance_final_step(encoded_projections, distribution_projections, p)\n",
    "    loss = reg - wasserstein_distance\n",
    "    f_op.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    f_op.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_1 == loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "inputs1 = torch.randn((32, 512))\n",
    "inputs2 = torch.randn((32, 512))\n",
    "em_q = F.normalize(inputs1, dim=1)\n",
    "em_k = F.normalize(inputs2, dim=1)\n",
    "sim_matrix = torch.matmul(em_q, em_k.t())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4713)\n",
      "tensor(3.4712)\n",
      "tensor(6.9425)\n"
     ]
    }
   ],
   "source": [
    "logpt1 = F.log_softmax(sim_matrix, dim=-1)\n",
    "logpt1 = torch.diag(logpt1)\n",
    "loss1 = -logpt1.mean()\n",
    "logpt2 = F.log_softmax(sim_matrix.T, dim=-1)\n",
    "logpt2 = torch.diag(logpt2)\n",
    "loss2 = -logpt2.mean()\n",
    "loss_1 = loss1 + loss2\n",
    "print(loss1)\n",
    "print(loss2)\n",
    "print(loss_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4713)\n",
      "tensor(3.4712)\n",
      "tensor(6.9425)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "loss1 = criterion(sim_matrix, torch.arange(0, inputs1.size(0)))\n",
    "loss2 = criterion(sim_matrix.t(), torch.arange(0, inputs1.size(0)))\n",
    "loss_2 = loss1 + loss2\n",
    "print(loss1)\n",
    "print(loss2)\n",
    "print(loss_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformNet(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs1 = torch.randn((32, 512)).cuda()\n",
    "inputs2 = torch.randn((32, 512)).cuda()\n",
    "transform_net = TransformNet(512)\n",
    "op_transnet = torch.optim.Adam(transform_net.parameters(), lr=0.0005, betas=(0.5, 0.999))\n",
    "temp1 = rand_projections(512, 1000)\n",
    "transform_net.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dswd = distributional_sliced_wasserstein_distance(inputs1.cuda(), inputs2.cuda(), 1024, transform_net, op_transnet, 2, 10, 1, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.5800, device='cuda:0', grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dswd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0362)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_distance_torch(temp1, temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_net = TransformNet(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "projections = transform_net(temp1)\n",
    "reg = 1 * cosine_distance_torch(projections, projections)\n",
    "encoded_projections = inputs1.matmul(projections.transpose(0, 1))\n",
    "distribution_projections = inputs2.matmul(projections.transpose(0, 1))\n",
    "wasserstein_distance = get_wasserstein_distance_final_step(encoded_projections, distribution_projections, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5076, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0662)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_moco_return_metrics_top_k(net, train_iter, val_iter, optimizer, epochs, device, tested_parameter, criterion=None,\n",
    "                                    k_candidates=(10,), scheduler=None):\n",
    "    # train_metrics = HistoryRecorder(['Train Loss', 'Train Acc', 'Val Loss', 'Val Acc'], [list, dict, list, dict])\n",
    "\n",
    "    target_tensor = get_CNI_tensor(TARGET_IMAGE, device=device)\n",
    "    train_loss_record = []\n",
    "    train_acc_record = {k: [] for k in k_candidates}\n",
    "    val_loss_record = []\n",
    "    val_acc_record = {k: [] for k in k_candidates}\n",
    "    for epoch in range(epochs):\n",
    "        net.cuda(device)\n",
    "        total_loss = 0\n",
    "        training_correct = collections.defaultdict(int)\n",
    "        training_size = 0\n",
    "        for origin, target, label in train_iter:\n",
    "            net.train()\n",
    "            total_loss += train_batch(net, origin,\n",
    "                                      target, label, optimizer, device=device)\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                for k, correct in zip(k_candidates,\n",
    "                                      cal_accuracy_top_k(image_pair_matching(net, origin.to(device), target_tensor), label.to(device),\n",
    "                                                         top_k=k_candidates)):\n",
    "                    training_correct[k] += correct\n",
    "                training_size += origin.shape[0]\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            val_correct = collections.defaultdict(int)\n",
    "            for origin, target, label in val_iter:\n",
    "                origin, target, label = origin.cuda(\n",
    "                    device), target.cuda(device), label.cuda(device)\n",
    "                # output, labels = net(origin, target, evaluate=True)\n",
    "                # val_loss += f.cross_entropy(output, labels).item()\n",
    "                em_q, em_k = net(origin, target)\n",
    "                sim_matrix = net.get_similarity_matrix(em_q, em_k)\n",
    "                val_loss += net.compute_loss(sim_matrix).item()\n",
    "                for k, correct in zip(k_candidates,\n",
    "                                      cal_accuracy_top_k(image_pair_matching(net, origin, target_tensor), label,\n",
    "                                                         top_k=k_candidates)):\n",
    "                    val_correct[k] += correct\n",
    "        val_acc = {k: correct / origin.shape[0]\n",
    "                   for k, correct in val_correct.items()}\n",
    "        train_acc = {k: correct / training_size for k,\n",
    "                     correct in training_correct.items()}\n",
    "        train_loss_record.append(total_loss / len(train_iter))\n",
    "        for k, v in train_acc.items():\n",
    "            train_acc_record[k].append(v)\n",
    "        val_loss_record.append(val_loss / len(val_iter))\n",
    "        for k, v in val_acc.items():\n",
    "            val_acc_record[k].append(v)\n",
    "        print(\n",
    "            f'Epoch {epoch + 1}, Train_Loss {total_loss / len(train_iter)}, Val_loss {val_loss / len(val_iter)}')\n",
    "        # for k, acc in train_acc.items():\n",
    "        #     print(f'Train_acc_top_{k} {round(acc, 4)}', end='\\t')\n",
    "        # print()\n",
    "        for k, acc in val_acc.items():\n",
    "            print(f'Val_acc_top_{k} {round(acc, 2)}', end='\\t')\n",
    "        print()\n",
    "    output = normalize_data_format(\n",
    "        {tuple(tested_parameter): (train_loss_record, train_acc_record, val_loss_record, val_acc_record)})\n",
    "    return output\n",
    "\n",
    "\n",
    "def normalize_data_format(data: dict, inner=False):\n",
    "    result = collections.defaultdict(list)\n",
    "    for k, v in data.items():\n",
    "        if isinstance(v, tuple):\n",
    "            for item in v:\n",
    "                if isinstance(item, list):\n",
    "                    result[k].append(np.array(item))\n",
    "                else:\n",
    "                    result[k].append(normalize_data_format(item, inner=True))\n",
    "        elif isinstance(v, list):\n",
    "            result[k].append(np.array(v))\n",
    "        elif isinstance(v, dict):\n",
    "            result[k].append(normalize_data_format(v, inner=True))\n",
    "    if inner:\n",
    "        for k, v in result.items():\n",
    "            if isinstance(v, list) and len(v) == 1:\n",
    "                result[k] = v[0]\n",
    "    return result\n",
    "\n",
    "\n",
    "def train_batch(net, observed, calculated, label, optimizer, criterion=None, device=None):\n",
    "    if device:\n",
    "        observed, calculated, label = observed.to(\n",
    "            device), calculated.to(device), label.to(device)\n",
    "    em_q, em_k = net(observed, calculated)\n",
    "    if criterion:\n",
    "        loss = criterion(em_q, em_k)\n",
    "    else:\n",
    "        sim_matrix = net.get_similarity_matrix(em_q, em_k)\n",
    "        loss = net.compute_loss(sim_matrix)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_candidates = (10, 20, 30)\n",
    "k = 7\n",
    "temps = 0.7\n",
    "momentums = 0.99\n",
    "k_value = 64\n",
    "parameters = {'epochs_pretrain_model': [400]}\n",
    "# parameters = {'pretrain_model': ['self_pretrained', 'CEM', 'ImageNet', 'None']}\n",
    "# parameters = {'pretrain_model': ['CEM']}\n",
    "train_metrics = HistoryRecorder(['Train Loss', 'Train Acc', 'Val Loss', 'Val Acc'], list(parameters.keys()))\n",
    "parameters = list(itertools.product(*parameters.values()))\n",
    "\n",
    "for i, parameter in enumerate(parameters):\n",
    "\n",
    "    ### custom part to get parameters\n",
    "    pretrain_model = parameter[0]\n",
    "    ### END\n",
    "    \n",
    "    for j, images in enumerate(k_fold_train_validation_split(ORIGINAL_IMAGE, TARGET_IMAGE, k)):\n",
    "        train_dataset = SingleChannelNDIDatasetContrastiveLearningWithAug(images, False)\n",
    "        val_dataset = SingleChannelNDIDatasetContrastiveLearningWithAug(images, True)\n",
    "        train_iter = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "        val_iter = DataLoader(val_dataset, batch_size=len(val_dataset))\n",
    "\n",
    "        model = get_self_pretrain_model(index=pretrain_model)\n",
    "        model = RetrievalModel(model)\n",
    "        model = model.cuda()\n",
    "        \n",
    "        \n",
    "        device = torch.device('cuda:0')\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=5e-3, momentum=0.9, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30, eta_min=5e-5, verbose=1)\n",
    "        start_time = time.time()\n",
    "        print(f'Parameter Index: {i} / {len(parameters)}, Fold Index: {j} / {k}')\n",
    "        metrics = train_moco_return_metrics_top_k(model, train_iter, val_iter, optimizer, 30, device,\n",
    "                                                    tested_parameter=parameter, k_candidates=top_k_candidates, scheduler=scheduler)\n",
    "        end_time = time.time()\n",
    "        train_metrics.cal_add(metrics)\n",
    "train_metrics.cal_divide(k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py2cuda118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
