{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as f\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from torchsummary import torchsummary\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "ORIGINAL_IMAGE = 'E:/datasets/NDI_images/Integreted/Observed'\n",
    "TARGET_IMAGE = 'E:/datasets/NDI_images/Integreted/Calculated'\n",
    "POSITIVE_RATIO = 0.1\n",
    "BATCH_SIZE = 8"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ORIGINAL_IMAGE = '/import/mqhome/duanct/dataSets/NDI_images/20220725/20220725/Observed_Crop_200x200pix'\n",
    "TARGET_IMAGE = '/import/mqhome/duanct/dataSets/NDI_images/20220725/20220725/Calculated_200x200/grayscale'\n",
    "POSITIVE_RATIO = 0.1\n",
    "BATCH_SIZE = 8"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CosineEmbeddingLoss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class NDI_dataset(Dataset):\n",
    "    def __init__(self, positive_ratio=0.5, times=10):\n",
    "        original_images = list(Path(ORIGINAL_IMAGE).glob('*.jpg'))\n",
    "        target_images = list(Path(TARGET_IMAGE).glob('*.jpg'))\n",
    "        self.origins, self.targets, self.label = [], [], []\n",
    "        for original_image in original_images:\n",
    "            self.origins.append(torchvision.io.read_image(str(original_image)))\n",
    "            self.targets.append(torchvision.io.read_image(str(Path.joinpath(Path(TARGET_IMAGE), original_image.name.split('_')[0] + '.jpg'))))\n",
    "            self.label.append(torch.tensor(1))\n",
    "        for _ in range(times):\n",
    "            for original_image in original_images:\n",
    "                self.origins.append(torchvision.io.read_image(str(original_image)))\n",
    "                self.targets.append(torchvision.io.read_image(str(np.random.choice(target_images))))\n",
    "                self.label.append(torch.tensor(-1))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.origins[idx], self.targets[idx], self.label[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.origins)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train_iter = DataLoader(NDI_dataset(POSITIVE_RATIO), BATCH_SIZE, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model.fc = nn.Linear(512, 256)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 100, 100]           3,136\n",
      "       BatchNorm2d-2         [-1, 64, 100, 100]             128\n",
      "              ReLU-3         [-1, 64, 100, 100]               0\n",
      "         MaxPool2d-4           [-1, 64, 50, 50]               0\n",
      "            Conv2d-5           [-1, 64, 50, 50]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 50, 50]             128\n",
      "              ReLU-7           [-1, 64, 50, 50]               0\n",
      "            Conv2d-8           [-1, 64, 50, 50]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 50, 50]             128\n",
      "             ReLU-10           [-1, 64, 50, 50]               0\n",
      "       BasicBlock-11           [-1, 64, 50, 50]               0\n",
      "           Conv2d-12           [-1, 64, 50, 50]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 50, 50]             128\n",
      "             ReLU-14           [-1, 64, 50, 50]               0\n",
      "           Conv2d-15           [-1, 64, 50, 50]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 50, 50]             128\n",
      "             ReLU-17           [-1, 64, 50, 50]               0\n",
      "       BasicBlock-18           [-1, 64, 50, 50]               0\n",
      "           Conv2d-19          [-1, 128, 25, 25]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 25, 25]             256\n",
      "             ReLU-21          [-1, 128, 25, 25]               0\n",
      "           Conv2d-22          [-1, 128, 25, 25]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 25, 25]             256\n",
      "           Conv2d-24          [-1, 128, 25, 25]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 25, 25]             256\n",
      "             ReLU-26          [-1, 128, 25, 25]               0\n",
      "       BasicBlock-27          [-1, 128, 25, 25]               0\n",
      "           Conv2d-28          [-1, 128, 25, 25]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 25, 25]             256\n",
      "             ReLU-30          [-1, 128, 25, 25]               0\n",
      "           Conv2d-31          [-1, 128, 25, 25]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 25, 25]             256\n",
      "             ReLU-33          [-1, 128, 25, 25]               0\n",
      "       BasicBlock-34          [-1, 128, 25, 25]               0\n",
      "           Conv2d-35          [-1, 256, 13, 13]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 13, 13]             512\n",
      "             ReLU-37          [-1, 256, 13, 13]               0\n",
      "           Conv2d-38          [-1, 256, 13, 13]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 13, 13]             512\n",
      "           Conv2d-40          [-1, 256, 13, 13]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 13, 13]             512\n",
      "             ReLU-42          [-1, 256, 13, 13]               0\n",
      "       BasicBlock-43          [-1, 256, 13, 13]               0\n",
      "           Conv2d-44          [-1, 256, 13, 13]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 13, 13]             512\n",
      "             ReLU-46          [-1, 256, 13, 13]               0\n",
      "           Conv2d-47          [-1, 256, 13, 13]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 13, 13]             512\n",
      "             ReLU-49          [-1, 256, 13, 13]               0\n",
      "       BasicBlock-50          [-1, 256, 13, 13]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 256]         131,328\n",
      "================================================================\n",
      "Total params: 11,301,568\n",
      "Trainable params: 11,301,568\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.15\n",
      "Forward/backward pass size (MB): 51.07\n",
      "Params size (MB): 43.11\n",
      "Estimated Total Size (MB): 94.34\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torchsummary.summary(model, input_size=(1, 200, 200), device='cpu')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Ave. Loss: 0.5940\n",
      "Epoch: 2, Ave. Loss: 0.5952\n",
      "Epoch: 3, Ave. Loss: 0.5944\n",
      "Epoch: 4, Ave. Loss: 0.5929\n",
      "Epoch: 5, Ave. Loss: 0.5938\n",
      "Epoch: 6, Ave. Loss: 0.5953\n",
      "Epoch: 7, Ave. Loss: 0.5931\n",
      "Epoch: 8, Ave. Loss: 0.5945\n",
      "Epoch: 9, Ave. Loss: 0.5908\n",
      "Epoch: 10, Ave. Loss: 0.5949\n",
      "Epoch: 11, Ave. Loss: 0.5910\n",
      "Epoch: 12, Ave. Loss: 0.5936\n",
      "Epoch: 13, Ave. Loss: 0.5947\n",
      "Epoch: 14, Ave. Loss: 0.5938\n",
      "Epoch: 15, Ave. Loss: 0.5946\n",
      "Epoch: 16, Ave. Loss: 0.5942\n",
      "Epoch: 17, Ave. Loss: 0.5944\n",
      "Epoch: 18, Ave. Loss: 0.5940\n",
      "Epoch: 19, Ave. Loss: 0.5937\n",
      "Epoch: 20, Ave. Loss: 0.5939\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "lr = 0.001\n",
    "EPOCHS = 20\n",
    "\n",
    "loss = nn.CosineEmbeddingLoss(margin=0.3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model.to(device)\n",
    "train_iter = DataLoader(NDI_dataset(POSITIVE_RATIO), BATCH_SIZE, shuffle=True)\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for origin, target, label in train_iter:\n",
    "        optimizer.zero_grad()\n",
    "        origin, target = origin.float(), target.float()\n",
    "        origin, target, label = origin.to(device), target.to(device), label.to(device)\n",
    "        origin_feature = model(origin)\n",
    "        target_feature = model(target)\n",
    "        l = loss(origin_feature, target_feature, label)\n",
    "        l.backward()\n",
    "        total_loss += l\n",
    "    print(f'Epoch: {epoch + 1}, Ave. Loss: {total_loss / len(train_iter):.4f}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9949], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "model.to('cpu')\n",
    "original_images = list(Path(ORIGINAL_IMAGE).glob('*.jpg'))\n",
    "original_image_name = np.random.choice(original_images)\n",
    "original_image = torchvision.io.read_image(str(original_image_name)).unsqueeze_(0).float()\n",
    "target_image = torchvision.io.read_image(str(Path.joinpath(Path(TARGET_IMAGE), original_image_name.name.split('_')[0] + '.jpg'))).unsqueeze_(0).float()\n",
    "print(nn.CosineSimilarity()(model(original_image), model(target_image)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9946], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "original_images = list(Path(ORIGINAL_IMAGE).glob('*.jpg'))\n",
    "target_images = list(Path(TARGET_IMAGE).glob('*.jpg'))\n",
    "original_image_name = np.random.choice(original_images)\n",
    "target_image_name = np.random.choice(target_images)\n",
    "original_image = torchvision.io.read_image(str(original_image_name)).unsqueeze_(0).float()\n",
    "target_image = torchvision.io.read_image(str(target_image_name)).unsqueeze_(0).float()\n",
    "print(nn.CosineSimilarity()(model(original_image), model(target_image)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.7585])"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimensions = 512\n",
    "a = torch.rand((1, dimensions))\n",
    "b = torch.rand((1, dimensions))\n",
    "nn.CosineSimilarity()(a, b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Binary Classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 512])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((8, 512))\n",
    "b = torch.rand((8, 512))\n",
    "torch.cat([a, b], dim=).size()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.backbone = torchvision.models.resnet18(pretrained=False)\n",
    "        self.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.backbone.fc = nn.Linear(512, 256)\n",
    "        self.classifier = nn.Linear(512, 2)\n",
    "    def forward(self, origin, target):\n",
    "        origin_feature = f.relu(self.backbone(origin))\n",
    "        target_feature = f.relu(self.backbone(target))\n",
    "        return self.classifier(torch.cat([origin_feature, target_feature], dim=-1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class NDI_dataset_classifier(Dataset):\n",
    "    def __init__(self, positive_ratio=0.5, times=10):\n",
    "        original_images = list(Path(ORIGINAL_IMAGE).glob('*.jpg'))\n",
    "        target_images = list(Path(TARGET_IMAGE).glob('*.jpg'))\n",
    "        self.origins, self.targets, self.label = [], [], []\n",
    "        for original_image in original_images:\n",
    "            self.origins.append(torchvision.io.read_image(str(original_image)))\n",
    "            self.targets.append(torchvision.io.read_image(str(Path.joinpath(Path(TARGET_IMAGE), original_image.name.split('_')[0] + '.jpg'))))\n",
    "            self.label.append(torch.tensor(1, dtype=torch.long))\n",
    "        for _ in range(times):\n",
    "            for original_image in original_images:\n",
    "                self.origins.append(torchvision.io.read_image(str(original_image)))\n",
    "                self.targets.append(torchvision.io.read_image(str(np.random.choice(target_images))))\n",
    "                self.label.append(torch.tensor(0, dtype=torch.long))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.origins[idx], self.targets[idx], self.label[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.origins)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 100, 100]           3,136\n",
      "       BatchNorm2d-2         [-1, 64, 100, 100]             128\n",
      "              ReLU-3         [-1, 64, 100, 100]               0\n",
      "         MaxPool2d-4           [-1, 64, 50, 50]               0\n",
      "            Conv2d-5           [-1, 64, 50, 50]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 50, 50]             128\n",
      "              ReLU-7           [-1, 64, 50, 50]               0\n",
      "            Conv2d-8           [-1, 64, 50, 50]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 50, 50]             128\n",
      "             ReLU-10           [-1, 64, 50, 50]               0\n",
      "       BasicBlock-11           [-1, 64, 50, 50]               0\n",
      "           Conv2d-12           [-1, 64, 50, 50]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 50, 50]             128\n",
      "             ReLU-14           [-1, 64, 50, 50]               0\n",
      "           Conv2d-15           [-1, 64, 50, 50]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 50, 50]             128\n",
      "             ReLU-17           [-1, 64, 50, 50]               0\n",
      "       BasicBlock-18           [-1, 64, 50, 50]               0\n",
      "           Conv2d-19          [-1, 128, 25, 25]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 25, 25]             256\n",
      "             ReLU-21          [-1, 128, 25, 25]               0\n",
      "           Conv2d-22          [-1, 128, 25, 25]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 25, 25]             256\n",
      "           Conv2d-24          [-1, 128, 25, 25]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 25, 25]             256\n",
      "             ReLU-26          [-1, 128, 25, 25]               0\n",
      "       BasicBlock-27          [-1, 128, 25, 25]               0\n",
      "           Conv2d-28          [-1, 128, 25, 25]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 25, 25]             256\n",
      "             ReLU-30          [-1, 128, 25, 25]               0\n",
      "           Conv2d-31          [-1, 128, 25, 25]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 25, 25]             256\n",
      "             ReLU-33          [-1, 128, 25, 25]               0\n",
      "       BasicBlock-34          [-1, 128, 25, 25]               0\n",
      "           Conv2d-35          [-1, 256, 13, 13]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 13, 13]             512\n",
      "             ReLU-37          [-1, 256, 13, 13]               0\n",
      "           Conv2d-38          [-1, 256, 13, 13]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 13, 13]             512\n",
      "           Conv2d-40          [-1, 256, 13, 13]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 13, 13]             512\n",
      "             ReLU-42          [-1, 256, 13, 13]               0\n",
      "       BasicBlock-43          [-1, 256, 13, 13]               0\n",
      "           Conv2d-44          [-1, 256, 13, 13]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 13, 13]             512\n",
      "             ReLU-46          [-1, 256, 13, 13]               0\n",
      "           Conv2d-47          [-1, 256, 13, 13]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 13, 13]             512\n",
      "             ReLU-49          [-1, 256, 13, 13]               0\n",
      "       BasicBlock-50          [-1, 256, 13, 13]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 256]         131,328\n",
      "           ResNet-69                  [-1, 256]               0\n",
      "           Conv2d-70         [-1, 64, 100, 100]           3,136\n",
      "      BatchNorm2d-71         [-1, 64, 100, 100]             128\n",
      "             ReLU-72         [-1, 64, 100, 100]               0\n",
      "        MaxPool2d-73           [-1, 64, 50, 50]               0\n",
      "           Conv2d-74           [-1, 64, 50, 50]          36,864\n",
      "      BatchNorm2d-75           [-1, 64, 50, 50]             128\n",
      "             ReLU-76           [-1, 64, 50, 50]               0\n",
      "           Conv2d-77           [-1, 64, 50, 50]          36,864\n",
      "      BatchNorm2d-78           [-1, 64, 50, 50]             128\n",
      "             ReLU-79           [-1, 64, 50, 50]               0\n",
      "       BasicBlock-80           [-1, 64, 50, 50]               0\n",
      "           Conv2d-81           [-1, 64, 50, 50]          36,864\n",
      "      BatchNorm2d-82           [-1, 64, 50, 50]             128\n",
      "             ReLU-83           [-1, 64, 50, 50]               0\n",
      "           Conv2d-84           [-1, 64, 50, 50]          36,864\n",
      "      BatchNorm2d-85           [-1, 64, 50, 50]             128\n",
      "             ReLU-86           [-1, 64, 50, 50]               0\n",
      "       BasicBlock-87           [-1, 64, 50, 50]               0\n",
      "           Conv2d-88          [-1, 128, 25, 25]          73,728\n",
      "      BatchNorm2d-89          [-1, 128, 25, 25]             256\n",
      "             ReLU-90          [-1, 128, 25, 25]               0\n",
      "           Conv2d-91          [-1, 128, 25, 25]         147,456\n",
      "      BatchNorm2d-92          [-1, 128, 25, 25]             256\n",
      "           Conv2d-93          [-1, 128, 25, 25]           8,192\n",
      "      BatchNorm2d-94          [-1, 128, 25, 25]             256\n",
      "             ReLU-95          [-1, 128, 25, 25]               0\n",
      "       BasicBlock-96          [-1, 128, 25, 25]               0\n",
      "           Conv2d-97          [-1, 128, 25, 25]         147,456\n",
      "      BatchNorm2d-98          [-1, 128, 25, 25]             256\n",
      "             ReLU-99          [-1, 128, 25, 25]               0\n",
      "          Conv2d-100          [-1, 128, 25, 25]         147,456\n",
      "     BatchNorm2d-101          [-1, 128, 25, 25]             256\n",
      "            ReLU-102          [-1, 128, 25, 25]               0\n",
      "      BasicBlock-103          [-1, 128, 25, 25]               0\n",
      "          Conv2d-104          [-1, 256, 13, 13]         294,912\n",
      "     BatchNorm2d-105          [-1, 256, 13, 13]             512\n",
      "            ReLU-106          [-1, 256, 13, 13]               0\n",
      "          Conv2d-107          [-1, 256, 13, 13]         589,824\n",
      "     BatchNorm2d-108          [-1, 256, 13, 13]             512\n",
      "          Conv2d-109          [-1, 256, 13, 13]          32,768\n",
      "     BatchNorm2d-110          [-1, 256, 13, 13]             512\n",
      "            ReLU-111          [-1, 256, 13, 13]               0\n",
      "      BasicBlock-112          [-1, 256, 13, 13]               0\n",
      "          Conv2d-113          [-1, 256, 13, 13]         589,824\n",
      "     BatchNorm2d-114          [-1, 256, 13, 13]             512\n",
      "            ReLU-115          [-1, 256, 13, 13]               0\n",
      "          Conv2d-116          [-1, 256, 13, 13]         589,824\n",
      "     BatchNorm2d-117          [-1, 256, 13, 13]             512\n",
      "            ReLU-118          [-1, 256, 13, 13]               0\n",
      "      BasicBlock-119          [-1, 256, 13, 13]               0\n",
      "          Conv2d-120            [-1, 512, 7, 7]       1,179,648\n",
      "     BatchNorm2d-121            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-122            [-1, 512, 7, 7]               0\n",
      "          Conv2d-123            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-124            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-125            [-1, 512, 7, 7]         131,072\n",
      "     BatchNorm2d-126            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-127            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-128            [-1, 512, 7, 7]               0\n",
      "          Conv2d-129            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-130            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-131            [-1, 512, 7, 7]               0\n",
      "          Conv2d-132            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-133            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-134            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-135            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-136            [-1, 512, 1, 1]               0\n",
      "          Linear-137                  [-1, 256]         131,328\n",
      "          ResNet-138                  [-1, 256]               0\n",
      "          Linear-139                    [-1, 2]           1,026\n",
      "================================================================\n",
      "Total params: 22,604,162\n",
      "Trainable params: 22,604,162\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 6103.52\n",
      "Forward/backward pass size (MB): 102.15\n",
      "Params size (MB): 86.23\n",
      "Estimated Total Size (MB): 6291.90\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = ClassificationModel()\n",
    "torchsummary.summary(model, input_size=[(1, 200, 200), (1, 200, 200)], device='cpu')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "lr = 0.001\n",
    "EPOCHS = 20\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model.to(device)\n",
    "train_iter = DataLoader(NDI_dataset(POSITIVE_RATIO), BATCH_SIZE, shuffle=True)\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for origin, target, label in train_iter:\n",
    "        optimizer.zero_grad()\n",
    "        origin, target = origin.float(), target.float()\n",
    "        origin, target, label = origin.to(device), target.to(device), label.to(device)\n",
    "        origin_feature = model(origin)\n",
    "        target_feature = model(target)\n",
    "        l = loss(origin_feature, target_feature, label)\n",
    "        l.backward()\n",
    "        total_loss += l\n",
    "    print(f'Epoch: {epoch + 1}, Ave. Loss: {total_loss / len(train_iter):.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "x = torch.rand((1, 2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(x).type(x.dtype).sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.6341])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "mnist_dataset = torchvision.datasets.FashionMNIST(root='../data', train=True, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor()]), download=True)\n",
    "mnist_loader = torch.utils.data.DataLoader(mnist_dataset, batch_size=32, shuffle=True)\n",
    "for x, y in mnist_loader:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.return_types.max(\nvalues=tensor([[0.7490, 0.9333, 0.8863, 0.8824, 0.8784, 0.8196, 0.8078, 0.8510, 0.8941,\n         0.9647, 0.9176, 1.0000, 0.8588, 0.9216, 0.9294, 0.9882, 0.9882, 0.9882,\n         0.9843, 0.9765, 0.9451, 0.9843, 0.9843, 0.9412, 0.8627, 0.9020, 0.8431,\n         0.6902]]),\nindices=tensor([[13, 15, 10, 16,  6, 18, 18,  7,  7, 19, 19, 19, 20,  9,  9,  6, 18, 18,\n         18, 18, 18, 21, 21,  8, 21, 19, 19, 22]]))"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(x[0], dim=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class NDIDatasetContrastiveLearning(Dataset):\n",
    "    def __init__(self):\n",
    "        super(NDIDatasetContrastiveLearning, self).__init__()\n",
    "        original_images = list(Path(ORIGINAL_IMAGE).glob('*.jpg'))\n",
    "        origins, targets, label = [], [], []\n",
    "        to_tensor_func = torchvision.transforms.ToTensor()\n",
    "        for original_image in original_images:\n",
    "            origins.append(to_tensor_func(Image.open(str(original_image))))\n",
    "            targets.append(to_tensor_func(Image.open(str(Path.joinpath(Path(TARGET_IMAGE), original_image.name.split('_')[0] + '.jpg')))))\n",
    "            label.append(int(original_image.name.split('_')[0]) - 1)\n",
    "        self.origins, self.targets, self.label = origins, targets, label\n",
    "        # random_index = np.random.permutation(len(origins))\n",
    "        # self.origins, self.targets, self.label = [], [], []\n",
    "        # for index in random_index:\n",
    "        #     self.origins.append(origins[index])\n",
    "        #     self.targets.append(targets[index])\n",
    "        #     self.label.append(index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.origins[idx], self.targets[idx], self.label[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.origins)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NDIDatasetContrastiveLearningDataAug(Dataset):\n",
    "    def __init__(self):\n",
    "        super(NDIDatasetContrastiveLearningDataAug, self).__init__()\n",
    "        original_images = list(Path(ORIGINAL_IMAGE).glob('*.jpg'))\n",
    "        origins, targets, label = [], [], []\n",
    "        self.to_tensor_func = torchvision.transforms.ToTensor()\n",
    "        for original_image in original_images:\n",
    "            original_img = Image.open(str(original_image))\n",
    "            target_img = Image.open(str(Path.joinpath(Path(TARGET_IMAGE), original_image.name.split('_')[0] + '.jpg')))\n",
    "            original_img, target_img = self.transform(original_img, target_img)\n",
    "            origins.append(original_img)\n",
    "            targets.append(target_img)\n",
    "            label.append(int(original_image.name.split('_')[0]) - 1)\n",
    "        self.origins, self.targets, self.label = origins, targets, label\n",
    "        # random_index = np.random.permutation(len(origins))\n",
    "        # self.origins, self.targets, self.label = [], [], []\n",
    "        # for index in random_index:\n",
    "        #     self.origins.append(origins[index])\n",
    "        #     self.targets.append(targets[index])\n",
    "        #     self.label.append(index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.origins[idx], self.targets[idx], self.label[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.origins)\n",
    "\n",
    "    def transform(self, img1, img2):\n",
    "        random_number = random.random()\n",
    "        if random_number < 0.33:\n",
    "            if random.random() < 0.5:\n",
    "                img1 = torchvision.transforms.functional.hflip(img1)\n",
    "                img2 = torchvision.transforms.functional.hflip(img2)\n",
    "            if random.random() < 0.5:\n",
    "                img1 = torchvision.transforms.functional.vflip(img1)\n",
    "                img2 = torchvision.transforms.functional.vflip(img2)\n",
    "        elif random_number < 0.7:\n",
    "            random_angle = torchvision.transforms.RandomRotation.get_params([-90, 90])\n",
    "            img1 = torchvision.transforms.functional.rotate(img1, random_angle)\n",
    "            img2 = torchvision.transforms.functional.rotate(img2, random_angle)\n",
    "        return self.to_tensor_func(img1), self.to_tensor_func(img2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from moco_model import MoCo\n",
    "\n",
    "\n",
    "model = MoCo(torchvision.models.resnet18, dim=128, K=40)\n",
    "device = torch.device('cuda:0')\n",
    "criterion = nn.CrossEntropyLoss().cuda(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=1e-4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "dataset = NDIDatasetContrastiveLearning()\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [40, 14])\n",
    "train_iter = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=True)\n",
    "val_iter = DataLoader(val_dataset, batch_size=len(val_dataset))\n",
    "to_tensor_func = torchvision.transforms.ToTensor()\n",
    "target_tensor = []\n",
    "for i in range(1, 55):\n",
    "    target_tensor.append(to_tensor_func(Image.open(str(Path.joinpath(Path(TARGET_IMAGE), f'{i}.jpg')))).unsqueeze(0))\n",
    "target_tensor = torch.cat(target_tensor, dim=0)\n",
    "target_tensor = target_tensor.cuda(device)\n",
    "model.cuda(device)\n",
    "for origin, _, label in val_iter:\n",
    "    origin, label = origin.cuda(device), label.cuda(device)\n",
    "    q = model.encoder_q(origin)\n",
    "    q = f.normalize(q, dim=1)\n",
    "    k = model.encoder_k(target_tensor)\n",
    "    k = f.normalize(k, dim=1)\n",
    "    logits = torch.mm(q, k.T)\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    print(torch.sum(preds.type_as(label) == label))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.9566, 0.9737, 0.9622, 0.9789, 0.9772, 0.9746, 0.9748, 0.9764, 0.9692,\n         0.9736, 0.9824, 0.9508, 0.9770, 0.9738, 0.9807, 0.8780, 0.9732, 0.9759,\n         0.9735, 0.9712, 0.9789, 0.9776, 0.9470, 0.9700, 0.9749, 0.9749, 0.9775,\n         0.8600, 0.8569, 0.9710, 0.9315, 0.9395, 0.9240, 0.9727, 0.9443, 0.9284,\n         0.9813, 0.9683, 0.9805, 0.9707, 0.8684, 0.9756, 0.9765, 0.8677, 0.8620,\n         0.9690, 0.9724, 0.9831, 0.9756, 0.9755, 0.9732, 0.8786, 0.8786, 0.9761],\n        [0.8980, 0.9588, 0.9742, 0.9485, 0.9552, 0.9592, 0.9695, 0.9412, 0.9699,\n         0.9617, 0.9651, 0.8908, 0.9637, 0.9763, 0.9623, 0.9404, 0.9670, 0.9623,\n         0.9509, 0.9670, 0.9708, 0.9638, 0.8875, 0.9633, 0.9731, 0.9731, 0.9593,\n         0.9233, 0.9180, 0.9660, 0.8786, 0.8786, 0.9607, 0.9524, 0.8836, 0.9681,\n         0.9570, 0.9758, 0.9712, 0.9782, 0.9272, 0.9494, 0.9666, 0.9345, 0.9131,\n         0.9709, 0.9676, 0.9566, 0.9366, 0.9362, 0.9623, 0.9421, 0.9421, 0.9698],\n        [0.8853, 0.9572, 0.9717, 0.9399, 0.9428, 0.9503, 0.9510, 0.9337, 0.9641,\n         0.9537, 0.9608, 0.8771, 0.9634, 0.9642, 0.9609, 0.9506, 0.9579, 0.9583,\n         0.9392, 0.9623, 0.9588, 0.9651, 0.8741, 0.9588, 0.9589, 0.9589, 0.9575,\n         0.9417, 0.9301, 0.9618, 0.8687, 0.8640, 0.9672, 0.9401, 0.8722, 0.9694,\n         0.9412, 0.9692, 0.9583, 0.9737, 0.9357, 0.9382, 0.9555, 0.9402, 0.9273,\n         0.9665, 0.9635, 0.9500, 0.9210, 0.9274, 0.9618, 0.9502, 0.9502, 0.9596],\n        [0.8944, 0.9643, 0.9727, 0.9530, 0.9581, 0.9546, 0.9655, 0.9454, 0.9691,\n         0.9704, 0.9665, 0.8914, 0.9615, 0.9747, 0.9628, 0.9437, 0.9689, 0.9650,\n         0.9535, 0.9711, 0.9673, 0.9655, 0.8859, 0.9693, 0.9695, 0.9695, 0.9668,\n         0.9342, 0.9249, 0.9738, 0.8758, 0.8741, 0.9636, 0.9455, 0.8819, 0.9725,\n         0.9536, 0.9771, 0.9689, 0.9752, 0.9360, 0.9524, 0.9675, 0.9350, 0.9237,\n         0.9736, 0.9685, 0.9604, 0.9351, 0.9381, 0.9616, 0.9390, 0.9390, 0.9738],\n        [0.9303, 0.9671, 0.9746, 0.9726, 0.9739, 0.9759, 0.9750, 0.9683, 0.9805,\n         0.9728, 0.9769, 0.9254, 0.9768, 0.9846, 0.9750, 0.9092, 0.9746, 0.9722,\n         0.9716, 0.9715, 0.9747, 0.9733, 0.9195, 0.9818, 0.9727, 0.9727, 0.9746,\n         0.9001, 0.8926, 0.9786, 0.9125, 0.9140, 0.9476, 0.9707, 0.9184, 0.9499,\n         0.9728, 0.9830, 0.9750, 0.9807, 0.9015, 0.9636, 0.9794, 0.8977, 0.8925,\n         0.9776, 0.9713, 0.9671, 0.9549, 0.9600, 0.9709, 0.9117, 0.9117, 0.9798],\n        [0.8850, 0.9597, 0.9719, 0.9462, 0.9476, 0.9386, 0.9607, 0.9345, 0.9728,\n         0.9698, 0.9637, 0.8784, 0.9586, 0.9615, 0.9612, 0.9521, 0.9645, 0.9653,\n         0.9487, 0.9628, 0.9700, 0.9606, 0.8781, 0.9635, 0.9696, 0.9696, 0.9702,\n         0.9428, 0.9367, 0.9741, 0.8639, 0.8705, 0.9697, 0.9373, 0.8749, 0.9681,\n         0.9463, 0.9735, 0.9646, 0.9754, 0.9439, 0.9401, 0.9567, 0.9464, 0.9382,\n         0.9697, 0.9667, 0.9518, 0.9331, 0.9266, 0.9538, 0.9492, 0.9492, 0.9696],\n        [0.9069, 0.9741, 0.9747, 0.9609, 0.9623, 0.9624, 0.9702, 0.9500, 0.9729,\n         0.9738, 0.9717, 0.9071, 0.9713, 0.9731, 0.9707, 0.9337, 0.9702, 0.9719,\n         0.9611, 0.9741, 0.9701, 0.9706, 0.9065, 0.9704, 0.9719, 0.9719, 0.9721,\n         0.9261, 0.9178, 0.9791, 0.8960, 0.8918, 0.9621, 0.9605, 0.9006, 0.9633,\n         0.9561, 0.9799, 0.9738, 0.9760, 0.9312, 0.9599, 0.9670, 0.9274, 0.9170,\n         0.9753, 0.9728, 0.9613, 0.9498, 0.9492, 0.9657, 0.9258, 0.9258, 0.9726],\n        [0.9153, 0.9777, 0.9783, 0.9628, 0.9653, 0.9622, 0.9703, 0.9595, 0.9742,\n         0.9759, 0.9746, 0.9129, 0.9770, 0.9776, 0.9720, 0.9314, 0.9710, 0.9744,\n         0.9665, 0.9728, 0.9784, 0.9747, 0.9069, 0.9719, 0.9750, 0.9750, 0.9786,\n         0.9220, 0.9137, 0.9820, 0.9003, 0.9019, 0.9612, 0.9642, 0.9044, 0.9602,\n         0.9651, 0.9778, 0.9780, 0.9802, 0.9200, 0.9560, 0.9712, 0.9230, 0.9124,\n         0.9817, 0.9763, 0.9669, 0.9512, 0.9541, 0.9715, 0.9283, 0.9283, 0.9774],\n        [0.9327, 0.9737, 0.9757, 0.9749, 0.9736, 0.9726, 0.9775, 0.9667, 0.9771,\n         0.9769, 0.9765, 0.9315, 0.9763, 0.9796, 0.9745, 0.9158, 0.9689, 0.9758,\n         0.9730, 0.9771, 0.9780, 0.9776, 0.9274, 0.9739, 0.9756, 0.9756, 0.9740,\n         0.9006, 0.8907, 0.9769, 0.9191, 0.9239, 0.9489, 0.9691, 0.9280, 0.9489,\n         0.9723, 0.9802, 0.9787, 0.9797, 0.9069, 0.9689, 0.9805, 0.9029, 0.8974,\n         0.9794, 0.9715, 0.9700, 0.9634, 0.9653, 0.9758, 0.9071, 0.9071, 0.9812],\n        [0.9449, 0.9702, 0.9661, 0.9752, 0.9736, 0.9693, 0.9766, 0.9709, 0.9726,\n         0.9724, 0.9811, 0.9370, 0.9744, 0.9704, 0.9798, 0.8975, 0.9714, 0.9789,\n         0.9708, 0.9798, 0.9744, 0.9750, 0.9347, 0.9720, 0.9742, 0.9742, 0.9765,\n         0.8858, 0.8820, 0.9757, 0.9257, 0.9273, 0.9370, 0.9682, 0.9290, 0.9387,\n         0.9754, 0.9724, 0.9813, 0.9692, 0.8858, 0.9717, 0.9706, 0.8901, 0.8786,\n         0.9713, 0.9745, 0.9794, 0.9657, 0.9659, 0.9715, 0.8990, 0.8990, 0.9734],\n        [0.9180, 0.9739, 0.9668, 0.9634, 0.9603, 0.9644, 0.9631, 0.9516, 0.9801,\n         0.9738, 0.9691, 0.9114, 0.9745, 0.9652, 0.9715, 0.9246, 0.9757, 0.9774,\n         0.9573, 0.9715, 0.9726, 0.9665, 0.9071, 0.9769, 0.9711, 0.9711, 0.9774,\n         0.9167, 0.9116, 0.9790, 0.8982, 0.8984, 0.9536, 0.9580, 0.9087, 0.9587,\n         0.9624, 0.9780, 0.9737, 0.9764, 0.9162, 0.9581, 0.9647, 0.9205, 0.9124,\n         0.9726, 0.9760, 0.9641, 0.9537, 0.9464, 0.9724, 0.9186, 0.9186, 0.9710],\n        [0.8818, 0.9631, 0.9747, 0.9448, 0.9452, 0.9454, 0.9571, 0.9338, 0.9681,\n         0.9663, 0.9648, 0.8777, 0.9621, 0.9641, 0.9622, 0.9573, 0.9655, 0.9679,\n         0.9444, 0.9693, 0.9664, 0.9618, 0.8705, 0.9616, 0.9660, 0.9660, 0.9662,\n         0.9492, 0.9407, 0.9737, 0.8666, 0.8645, 0.9716, 0.9392, 0.8684, 0.9782,\n         0.9438, 0.9766, 0.9648, 0.9779, 0.9515, 0.9406, 0.9630, 0.9520, 0.9433,\n         0.9735, 0.9705, 0.9521, 0.9287, 0.9274, 0.9558, 0.9536, 0.9536, 0.9706],\n        [0.8882, 0.9617, 0.9743, 0.9514, 0.9526, 0.9486, 0.9626, 0.9403, 0.9724,\n         0.9694, 0.9671, 0.8867, 0.9636, 0.9721, 0.9629, 0.9423, 0.9702, 0.9694,\n         0.9491, 0.9675, 0.9736, 0.9676, 0.8784, 0.9687, 0.9744, 0.9744, 0.9689,\n         0.9336, 0.9319, 0.9767, 0.8666, 0.8676, 0.9648, 0.9464, 0.8763, 0.9739,\n         0.9482, 0.9816, 0.9664, 0.9776, 0.9420, 0.9483, 0.9670, 0.9378, 0.9332,\n         0.9750, 0.9684, 0.9581, 0.9347, 0.9335, 0.9563, 0.9445, 0.9445, 0.9741],\n        [0.8957, 0.9667, 0.9743, 0.9557, 0.9603, 0.9562, 0.9643, 0.9409, 0.9804,\n         0.9722, 0.9671, 0.8893, 0.9712, 0.9709, 0.9685, 0.9471, 0.9661, 0.9694,\n         0.9577, 0.9727, 0.9726, 0.9688, 0.8826, 0.9768, 0.9721, 0.9721, 0.9659,\n         0.9439, 0.9324, 0.9788, 0.8727, 0.8740, 0.9717, 0.9509, 0.8796, 0.9775,\n         0.9567, 0.9827, 0.9698, 0.9824, 0.9450, 0.9475, 0.9685, 0.9425, 0.9401,\n         0.9773, 0.9723, 0.9549, 0.9363, 0.9403, 0.9617, 0.9456, 0.9456, 0.9756]],\n       device='cuda:0', grad_fn=<ViewBackward>)"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_neg = torch.einsum('nc,ck->nk', [q, k.T])\n",
    "l_neg"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "def train_moco(net, criterion, optimizer, epochs, device):\n",
    "    dataset = NDIDatasetContrastiveLearning()\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [40, 14])\n",
    "    train_iter = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=True)\n",
    "    val_iter = DataLoader(val_dataset, batch_size=len(val_dataset))\n",
    "    to_tensor_func = torchvision.transforms.ToTensor()\n",
    "    target_tensor = []\n",
    "    for i in range(1, 55):\n",
    "        target_tensor.append(to_tensor_func(Image.open(str(Path.joinpath(Path(TARGET_IMAGE), f'{i}.jpg')))).unsqueeze(0))\n",
    "    target_tensor = torch.cat(target_tensor, dim=0)\n",
    "    target_tensor = target_tensor.cuda(device)\n",
    "    for epoch in range(epochs):\n",
    "        net.cuda(device)\n",
    "        total_loss = 0\n",
    "        training_correct = 0\n",
    "        training_size = 0\n",
    "        for origin, target, label in train_iter:\n",
    "            net.train()\n",
    "            origin, target, label = origin.cuda(device), target.cuda(device), label.cuda(device)\n",
    "            output, labels = net(origin, target)\n",
    "            loss = criterion(output, labels)\n",
    "            total_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            net.eval()\n",
    "            training_correct += cal_accuracy(image_pair_matching(net, origin, target_tensor), label)\n",
    "            training_size += origin.shape[0]\n",
    "        net.eval()\n",
    "        for origin, _, label in val_iter:\n",
    "            origin, label = origin.cuda(device), label.cuda(device)\n",
    "            val_correct = cal_accuracy(image_pair_matching(net, origin, target_tensor), label)\n",
    "        val_acc = val_correct / origin.shape[0]\n",
    "        print(f'Epoch {epoch + 1}, Loss {total_loss / len(train_iter)}, Train_acc {training_correct / training_size}, Val_acc {val_acc}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1.7299450635910034, Train_acc 0.7, Val_acc 0.7857142857142857\n",
      "Epoch 2, Loss 1.6755859851837158, Train_acc 0.7, Val_acc 0.7857142857142857\n",
      "Epoch 3, Loss 1.4713821172714234, Train_acc 0.625, Val_acc 0.7857142857142857\n",
      "Epoch 4, Loss 1.4476009130477905, Train_acc 0.85, Val_acc 0.7857142857142857\n",
      "Epoch 5, Loss 1.4682427644729614, Train_acc 0.9, Val_acc 0.7857142857142857\n",
      "Epoch 6, Loss 1.4163634061813355, Train_acc 0.825, Val_acc 0.7142857142857143\n",
      "Epoch 7, Loss 1.3091506481170654, Train_acc 0.95, Val_acc 0.7857142857142857\n",
      "Epoch 8, Loss 1.2512558221817016, Train_acc 0.95, Val_acc 0.7857142857142857\n",
      "Epoch 9, Loss 1.224940299987793, Train_acc 0.95, Val_acc 0.7857142857142857\n",
      "Epoch 10, Loss 1.2253174304962158, Train_acc 0.95, Val_acc 0.7857142857142857\n",
      "Epoch 11, Loss 1.245502781867981, Train_acc 0.95, Val_acc 0.7857142857142857\n",
      "Epoch 12, Loss 1.213822364807129, Train_acc 0.95, Val_acc 0.7857142857142857\n",
      "Epoch 13, Loss 1.230686330795288, Train_acc 0.95, Val_acc 0.7857142857142857\n",
      "Epoch 14, Loss 1.1425687074661255, Train_acc 0.95, Val_acc 0.7857142857142857\n",
      "Epoch 15, Loss 1.1543770790100099, Train_acc 0.95, Val_acc 0.7857142857142857\n",
      "Epoch 16, Loss 1.0890823960304261, Train_acc 0.95, Val_acc 0.7857142857142857\n",
      "Epoch 17, Loss 1.1371859788894654, Train_acc 0.95, Val_acc 0.7857142857142857\n",
      "Epoch 18, Loss 1.0381474614143371, Train_acc 0.95, Val_acc 0.7857142857142857\n",
      "Epoch 19, Loss 1.0952553868293762, Train_acc 0.95, Val_acc 0.7857142857142857\n",
      "Epoch 20, Loss 1.022648274898529, Train_acc 0.95, Val_acc 0.7857142857142857\n",
      "Epoch 21, Loss 1.029719591140747, Train_acc 0.95, Val_acc 0.7857142857142857\n",
      "Epoch 22, Loss 0.995955228805542, Train_acc 0.95, Val_acc 0.7857142857142857\n"
     ]
    }
   ],
   "source": [
    "# 把K设置成48还算成功，在queue不大的情况下\n",
    "\n",
    "train_moco(model, criterion, optimizer, 50, device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 2.673976675757634\n",
      "Epoch 2, Loss 3.931485931078593\n",
      "Epoch 3, Loss 3.8541926542917886\n",
      "Epoch 4, Loss 3.6459306478500366\n",
      "Epoch 5, Loss 3.407223383585612\n",
      "Epoch 6, Loss 3.105238596598307\n",
      "Epoch 7, Loss 3.1296446720759072\n",
      "Epoch 8, Loss 3.1514963706334433\n",
      "Epoch 9, Loss 3.053861141204834\n",
      "Epoch 10, Loss 2.6810205380121865\n",
      "Epoch 11, Loss 2.705527981122335\n",
      "Epoch 12, Loss 2.632307688395182\n",
      "Epoch 13, Loss 2.4753270546595254\n",
      "Epoch 14, Loss 2.3976933558781943\n",
      "Epoch 15, Loss 2.333985129992167\n",
      "Epoch 16, Loss 2.2366109689076743\n",
      "Epoch 17, Loss 2.1844319701194763\n",
      "Epoch 18, Loss 2.2525221506754556\n",
      "Epoch 19, Loss 2.1583401759465537\n",
      "Epoch 20, Loss 2.0467366774876914\n",
      "Epoch 21, Loss 2.0968031088511148\n",
      "Epoch 22, Loss 2.081486225128174\n",
      "Epoch 23, Loss 2.047618548075358\n",
      "Epoch 24, Loss 1.9676475127538045\n",
      "Epoch 25, Loss 1.9295819004376729\n",
      "Epoch 26, Loss 1.8663320342699687\n",
      "Epoch 27, Loss 1.8291430870691936\n",
      "Epoch 28, Loss 1.789828399817149\n",
      "Epoch 29, Loss 1.7487680315971375\n",
      "Epoch 30, Loss 1.7118704319000244\n",
      "Epoch 31, Loss 1.761828859647115\n",
      "Epoch 32, Loss 1.7304572860399883\n",
      "Epoch 33, Loss 1.703533907731374\n",
      "Epoch 34, Loss 1.552752415339152\n",
      "Epoch 35, Loss 1.698424796263377\n",
      "Epoch 36, Loss 1.6368554035822551\n",
      "Epoch 37, Loss 1.6863943934440613\n",
      "Epoch 38, Loss 1.5227134029070537\n",
      "Epoch 39, Loss 1.5379104018211365\n",
      "Epoch 40, Loss 1.5471089084943135\n",
      "Epoch 41, Loss 1.547522524992625\n",
      "Epoch 42, Loss 1.439075231552124\n",
      "Epoch 43, Loss 1.495581865310669\n",
      "Epoch 44, Loss 1.424798885981242\n",
      "Epoch 45, Loss 1.386127809683482\n",
      "Epoch 46, Loss 1.3655091325441997\n",
      "Epoch 47, Loss 1.366443653901418\n",
      "Epoch 48, Loss 1.3557376265525818\n",
      "Epoch 49, Loss 1.3533420364061992\n",
      "Epoch 50, Loss 1.2456497550010681\n"
     ]
    }
   ],
   "source": [
    "# 尝试把K设置成56\n",
    "\n",
    "model = MoCo(torchvision.models.resnet18, dim=128, K=56)\n",
    "device = torch.device('cuda:0')\n",
    "criterion = nn.CrossEntropyLoss().cuda(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=1e-4)\n",
    "train_moco(model, criterion, optimizer, 50, device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def image_pair_matching(net, original_image, matching_image):\n",
    "    net.eval()\n",
    "    q = net.encoder_q(original_image)\n",
    "    q = f.normalize(q, dim=1)\n",
    "    k = net.encoder_k(matching_image)\n",
    "    k = f.normalize(k, dim=1)\n",
    "    logits = torch.einsum('nc,ck->nk', [q, k.T])\n",
    "    return logits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def cal_accuracy(preds, label):\n",
    "    return float(torch.sum(torch.argmax(preds, dim=1).type_as(label) == label))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "# 取总共大小小于K的queue来试，成功\n",
    "\n",
    "model.cpu()\n",
    "original_images = list(Path(ORIGINAL_IMAGE).glob('*.jpg'))\n",
    "target_images = list(Path(TARGET_IMAGE).glob('*.jpg'))\n",
    "to_tensor_func = torchvision.transforms.ToTensor()\n",
    "original_image = np.random.choice(original_images)\n",
    "original_tensor = to_tensor_func(Image.open(str(original_image))).unsqueeze(0)\n",
    "target_images_candidates = np.random.choice(target_images, 40)\n",
    "target_tensor = []\n",
    "for i in range(20):\n",
    "    target_tensor.append(to_tensor_func(Image.open(str(target_images_candidates[i]))).unsqueeze(0))\n",
    "target_tensor.append(to_tensor_func(Image.open(str(Path.joinpath(Path(TARGET_IMAGE), original_image.name.split('_')[0] + '.jpg')))).unsqueeze(0))\n",
    "for i in range(20, 40):\n",
    "    target_tensor.append(to_tensor_func(Image.open(str(target_images_candidates[i]))).unsqueeze(0))\n",
    "target_tensor = torch.cat(target_tensor, dim=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.2281,  0.1505,  0.0759,  0.3091,  0.0944,  0.3091, -0.0131,  0.1535,\n          0.0576,  0.1025,  0.0797,  0.1233,  0.1377,  0.0640,  0.1535,  0.0891,\n         -0.0098,  0.0383,  0.2141, -0.0383,  0.3091,  0.0969,  0.0739,  0.0576,\n          0.1209,  0.1377,  0.0891,  0.1424,  0.0175,  0.1025,  0.1732,  0.0883,\n          0.0811,  0.0613,  0.1233, -0.0235,  0.1025,  0.0883,  0.0738,  0.2119,\n          0.0383]], grad_fn=<MmBackward>)"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pair_matching(model, original_tensor, target_tensor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "# 取所有比对图片\n",
    "\n",
    "model.cpu()\n",
    "original_images = list(Path(ORIGINAL_IMAGE).glob('*.jpg'))\n",
    "target_images = list(Path(TARGET_IMAGE).glob('*.jpg'))\n",
    "to_tensor_func = torchvision.transforms.ToTensor()\n",
    "original_image = np.random.choice(original_images)\n",
    "original_tensor = to_tensor_func(Image.open(str(original_image))).unsqueeze(0)\n",
    "target_tensor = []\n",
    "for i in range(1, 55):\n",
    "    target_tensor.append(to_tensor_func(Image.open(str(Path.joinpath(Path(TARGET_IMAGE), f'{i}.jpg')))).unsqueeze(0))\n",
    "target_tensor = torch.cat(target_tensor, dim=0)\n",
    "print(str(original_image.name).split('_')[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4565e-01,  1.1097e-01,  3.9617e-02,  1.2304e-01,  1.0800e-01,\n",
      "          1.1116e-01,  1.9075e-01,  1.6977e-01,  7.8928e-02,  2.5626e-01,\n",
      "          9.6446e-02,  1.6837e-01,  8.6016e-02,  1.0304e-01,  9.4899e-02,\n",
      "          3.0067e-02,  8.0622e-02,  1.3401e-01,  8.8296e-02, -1.2011e-03,\n",
      "          4.0938e-01,  5.5814e-02,  1.7832e-01,  6.8754e-02,  3.9356e-01,\n",
      "          3.9356e-01,  2.4748e-01, -9.8140e-02, -9.8763e-02,  1.2288e-01,\n",
      "          1.8330e-01,  1.7046e-01,  7.5117e-02,  1.4688e-01,  1.3979e-01,\n",
      "          1.2716e-04,  1.2593e-01,  7.9922e-02,  3.0441e-01,  3.9980e-02,\n",
      "          2.0772e-02,  1.7252e-01,  1.3889e-01,  3.5146e-03,  3.6427e-02,\n",
      "          1.8339e-01,  8.3577e-02,  5.3283e-02,  1.7072e-01,  1.6657e-01,\n",
      "          1.9643e-01,  5.9414e-02,  5.9414e-02,  1.3753e-01]],\n",
      "       grad_fn=<MmBackward>)\n",
      "tensor(20)\n"
     ]
    }
   ],
   "source": [
    "logits = image_pair_matching(model, original_tensor, target_tensor)\n",
    "print(logits)\n",
    "print(torch.argmax(logits))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 200, 200])\n",
      "torch.Size([1, 1, 200, 200])\n"
     ]
    }
   ],
   "source": [
    "dataset = NDIDatasetContrastiveLearning()\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [43, 11])\n",
    "train_iter = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=True)\n",
    "val_iter = DataLoader(val_dataset, batch_size=1)\n",
    "for original, target in val_iter:\n",
    "    print(original.shape)\n",
    "    print(target.shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def train_moco_no_val(net, criterion, optimizer, epochs, device):\n",
    "    dataset = NDIDatasetContrastiveLearning()\n",
    "    train_iter = DataLoader(dataset, batch_size=8, shuffle=True, drop_last=True)\n",
    "    to_tensor_func = torchvision.transforms.ToTensor()\n",
    "    target_tensor = []\n",
    "    for i in range(1, 55):\n",
    "        target_tensor.append(to_tensor_func(Image.open(str(Path.joinpath(Path(TARGET_IMAGE), f'{i}.jpg')))).unsqueeze(0))\n",
    "    target_tensor = torch.cat(target_tensor, dim=0)\n",
    "    target_tensor = target_tensor.cuda(device)\n",
    "    for epoch in range(epochs):\n",
    "        net.cuda(device)\n",
    "        total_loss = 0\n",
    "        training_correct = 0\n",
    "        training_size = 0\n",
    "        for origin, target, label in train_iter:\n",
    "            net.train()\n",
    "            origin, target, label = origin.cuda(device), target.cuda(device), label.cuda(device)\n",
    "            output, labels = net(origin, target)\n",
    "            loss = criterion(output, labels)\n",
    "            total_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            net.eval()\n",
    "            training_correct += cal_accuracy(image_pair_matching(net, origin, target_tensor), label)\n",
    "            training_size += origin.shape[0]\n",
    "        print(f'Epoch {epoch + 1}, Loss {total_loss / len(train_iter)}, Train_acc {training_correct / training_size}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 2.5786702330660773, Train_acc 0.020833333333333332\n",
      "Epoch 2, Loss 3.732647697130839, Train_acc 0.020833333333333332\n",
      "Epoch 3, Loss 3.5257873932520547, Train_acc 0.041666666666666664\n",
      "Epoch 4, Loss 3.495723605155945, Train_acc 0.020833333333333332\n",
      "Epoch 5, Loss 3.1676207780838013, Train_acc 0.020833333333333332\n",
      "Epoch 6, Loss 2.9918357928593955, Train_acc 0.020833333333333332\n",
      "Epoch 7, Loss 2.993980884552002, Train_acc 0.020833333333333332\n",
      "Epoch 8, Loss 3.0280898809432983, Train_acc 0.0\n",
      "Epoch 9, Loss 2.756258209546407, Train_acc 0.020833333333333332\n",
      "Epoch 10, Loss 2.701042890548706, Train_acc 0.0\n",
      "Epoch 11, Loss 2.528733491897583, Train_acc 0.10416666666666667\n",
      "Epoch 12, Loss 2.5128378868103027, Train_acc 0.22916666666666666\n",
      "Epoch 13, Loss 2.264402707417806, Train_acc 0.4791666666666667\n",
      "Epoch 14, Loss 2.3837443590164185, Train_acc 0.4166666666666667\n",
      "Epoch 15, Loss 2.1378223299980164, Train_acc 0.6458333333333334\n",
      "Epoch 16, Loss 2.1182019114494324, Train_acc 0.7291666666666666\n",
      "Epoch 17, Loss 2.0684349139531455, Train_acc 0.8541666666666666\n",
      "Epoch 18, Loss 1.9607265790303547, Train_acc 0.9166666666666666\n",
      "Epoch 19, Loss 1.979739209016164, Train_acc 0.8958333333333334\n",
      "Epoch 20, Loss 1.8771478335062664, Train_acc 0.9583333333333334\n",
      "Epoch 21, Loss 1.9148467580477397, Train_acc 0.9166666666666666\n",
      "Epoch 22, Loss 1.7822499871253967, Train_acc 0.9166666666666666\n",
      "Epoch 23, Loss 1.7876503467559814, Train_acc 0.8541666666666666\n",
      "Epoch 24, Loss 1.6607769131660461, Train_acc 0.9583333333333334\n",
      "Epoch 25, Loss 1.7927319606145222, Train_acc 0.7291666666666666\n",
      "Epoch 26, Loss 1.7374668916066487, Train_acc 0.9166666666666666\n",
      "Epoch 27, Loss 1.7325751185417175, Train_acc 0.9583333333333334\n",
      "Epoch 28, Loss 1.6894067923227947, Train_acc 0.875\n",
      "Epoch 29, Loss 1.6757354537645976, Train_acc 0.9583333333333334\n",
      "Epoch 30, Loss 1.5410806735356648, Train_acc 0.9166666666666666\n",
      "Epoch 31, Loss 1.6231820980707805, Train_acc 0.9375\n",
      "Epoch 32, Loss 1.5019185145696003, Train_acc 0.9583333333333334\n",
      "Epoch 33, Loss 1.5377275149027507, Train_acc 0.9375\n",
      "Epoch 34, Loss 1.4896324276924133, Train_acc 0.9375\n",
      "Epoch 35, Loss 1.4161716898282368, Train_acc 0.9583333333333334\n",
      "Epoch 36, Loss 1.4720125198364258, Train_acc 0.9583333333333334\n",
      "Epoch 37, Loss 1.3738938172658284, Train_acc 0.9375\n",
      "Epoch 38, Loss 1.377350926399231, Train_acc 0.9375\n",
      "Epoch 39, Loss 1.3796300490697224, Train_acc 0.9583333333333334\n",
      "Epoch 40, Loss 1.3069811065991719, Train_acc 0.9791666666666666\n",
      "Epoch 41, Loss 1.3425143361091614, Train_acc 0.9583333333333334\n",
      "Epoch 42, Loss 1.3208018938700359, Train_acc 0.9583333333333334\n",
      "Epoch 43, Loss 1.3727254470189412, Train_acc 0.9583333333333334\n",
      "Epoch 44, Loss 1.2306936780611675, Train_acc 0.9583333333333334\n",
      "Epoch 45, Loss 1.208016832669576, Train_acc 0.9583333333333334\n",
      "Epoch 46, Loss 1.2964904109636943, Train_acc 0.9583333333333334\n",
      "Epoch 47, Loss 1.23629625638326, Train_acc 0.9583333333333334\n",
      "Epoch 48, Loss 1.2906123797098796, Train_acc 0.9583333333333334\n",
      "Epoch 49, Loss 1.132289469242096, Train_acc 0.9791666666666666\n",
      "Epoch 50, Loss 1.3014224569002788, Train_acc 0.9375\n",
      "Cost 156.94521617889404 secs\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_moco_no_val(model, criterion, optimizer, 50, device)\n",
    "end_time = time.time()\n",
    "print(f'Cost {end_time - start_time} secs')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "train_iter = DataLoader(NDIDatasetContrastiveLearning(), batch_size=8, shuffle=True, drop_last=True)\n",
    "x1, x2, x3, y1, y2, y3 = [], [], [], [], [], []\n",
    "for origin, target, label in train_iter:\n",
    "    x1.append(origin)\n",
    "    x2.append(target)\n",
    "    x3.append(label)\n",
    "for origin, target, label in train_iter:\n",
    "    y1.append(origin)\n",
    "    y2.append(target)\n",
    "    y3.append(label)\n",
    "x1 = torch.cat(x1, dim=0).squeeze(1)\n",
    "y1 = torch.cat(y1, dim=0).squeeze(1)\n",
    "for i in range(x1.shape[0]):\n",
    "    flag = False\n",
    "    for j in range(y1.shape[0]):\n",
    "        if (x1[i] == y1[j]).sum() == 40000:\n",
    "            print(True)\n",
    "            flag = True\n",
    "            break\n",
    "    if not flag:\n",
    "        print(False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# rename part\n",
    "\n",
    "rename_path = Path('E:/datasets/NDI_images/20220917_ElectronDiffraction-20220919T162109Z-001/20220917_ElectronDiffraction/Obs_200x200_jpg')\n",
    "files =rename_path.glob('*.jpg')\n",
    "for file in files:\n",
    "    new_name = str(file.name).split('_')\n",
    "    new_name[0] = str(int(new_name[0]) + 59)\n",
    "    new_name = '_'.join(new_name)\n",
    "    file.rename(str(Path.joinpath(rename_path, new_name)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "rename_path = Path('E:/datasets/NDI_images/20220917_ElectronDiffraction-20220919T162109Z-001/20220917_ElectronDiffraction/Cal_200x200_png')\n",
    "files = rename_path.glob('*.png')\n",
    "files = sorted(list(files), key=lambda x: int(str(x.name).split('.')[0]), reverse=True)\n",
    "for file in files:\n",
    "    new_name = str(file.name).split('.')\n",
    "    new_name[0] = str(int(new_name[0]) + 59)\n",
    "    new_name = '.'.join(new_name)\n",
    "    file.rename(str(Path.joinpath(rename_path, new_name)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
