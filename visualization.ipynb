{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from dataset import SingleChannelNDIDatasetContrastiveLearningWithAug\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor(img_tensor: torch.Tensor, num_img_per_row: int=6, figsize:Union[tuple, list]=(12, 12)):\n",
    "    num_channels = img_tensor.size(0)\n",
    "    n_rows = num_channels // num_img_per_row\n",
    "    fig, axes = plt.subplots(n_rows, num_img_per_row, figsize=figsize)\n",
    "    for i in range(n_rows):\n",
    "        for j in range(num_img_per_row):\n",
    "            idx = i * num_img_per_row + j\n",
    "            if idx < num_channels:\n",
    "                tensor = img_tensor[idx]\n",
    "                assert tensor.dim() in (2, 3)\n",
    "                img = tensor.numpy()\n",
    "                if tensor.dim() == 3:\n",
    "                    img = img.transpose(1, 2, 0)\n",
    "                axes[i, j].imshow(img)\n",
    "                axes[i, j].axis('off')\n",
    "            else:\n",
    "                axes[i, j].remove()\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=-0.8)\n",
    "    plt.show()\n",
    "\n",
    "def get_model():\n",
    "    base_encoder = torchvision.models.resnet50(weights=None)\n",
    "    base_encoder.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    origin_dim_mlp = base_encoder.fc.in_features\n",
    "    base_encoder.fc = None\n",
    "    temp = torch.load(f'./checkpoints/ImageNet_ALL_CHECK_400_Epoch.pth')['state_dict']\n",
    "    state_dict = {}\n",
    "    for k, v in temp.items():\n",
    "        if 'encoder_q' in k:\n",
    "            if 'fc' not in k:\n",
    "                state_dict['.'.join(k.split('.')[1:])] = v\n",
    "    base_encoder.load_state_dict(state_dict)\n",
    "    base_encoder.fc = torch.nn.Linear(origin_dim_mlp, 512)\n",
    "    return base_encoder\n",
    "\n",
    "def get_intermediate_output_ResNet(model: torchvision.models.ResNet, inputs: torch.Tensor):\n",
    "    x = inputs\n",
    "    output = []\n",
    "    for i, layer in enumerate(list(model.children())):\n",
    "        x = layer(x)\n",
    "        if i in (4, 5, 6, 7):\n",
    "            output.append(x)\n",
    "        if i == 8:\n",
    "            x = torch.flatten(x, 1)\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gp.sc.cc.tohoku.ac.jp/duanct/miniconda3/envs/py2cuda118/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 100, 100])\n",
      "torch.Size([1, 64, 100, 100])\n",
      "torch.Size([1, 64, 100, 100])\n",
      "torch.Size([1, 64, 50, 50])\n",
      "torch.Size([1, 256, 50, 50])\n",
      "torch.Size([1, 512, 25, 25])\n",
      "torch.Size([1, 1024, 13, 13])\n",
      "torch.Size([1, 2048, 7, 7])\n",
      "torch.Size([1, 2048, 1, 1])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 64, 100, 100])\n",
      "torch.Size([1, 64, 100, 100])\n",
      "torch.Size([1, 64, 100, 100])\n",
      "torch.Size([1, 64, 50, 50])\n",
      "torch.Size([1, 256, 50, 50])\n",
      "torch.Size([1, 512, 25, 25])\n",
      "torch.Size([1, 1024, 13, 13])\n",
      "torch.Size([1, 2048, 7, 7])\n",
      "torch.Size([1, 2048, 1, 1])\n",
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "ORIGINAL_IMAGE = '../datasets/NDI_images/Integreted/Observed/'\n",
    "TARGET_IMAGE = '../datasets/NDI_images/Integreted/Calculated/'\n",
    "original_images = list(sorted(list(map(str, list(Path(ORIGINAL_IMAGE).glob('*.jpg'))))))\n",
    "target_images = list(sorted(list(map(str, list(Path(TARGET_IMAGE).glob('*.jpg'))))))\n",
    "images = list(reversed(list(zip(original_images, target_images))))\n",
    "train_dataset = SingleChannelNDIDatasetContrastiveLearningWithAug([images, []], False, 200)\n",
    "train_iter = DataLoader(train_dataset, 1, shuffle=True, drop_last=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    origin, target, label = next(iter(train_iter))\n",
    "    model = get_model()\n",
    "    origin_inter, target_inter = get_intermediate_output_ResNet(model, origin), get_intermediate_output_ResNet(model, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_tensor(origin_inter[3][0][:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_tensor(target_inter[3][0][:12])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py2cuda118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
