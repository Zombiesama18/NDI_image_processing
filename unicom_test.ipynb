{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicom\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.load('./checkpoints/UNICOM_ViT_B_32_based.pth')['state_dict']\n",
    "state_dict = {}\n",
    "for k, v in temp.items():\n",
    "    if 'encoder_q' in k:\n",
    "        state_dict['.'.join(k.split('.')[1:])] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['queue', 'queue_ptr', 'encoder_q.pos_embed', 'encoder_q.patch_embed.proj.weight', 'encoder_q.patch_embed.proj.bias', 'encoder_q.blocks.0.norm1.weight', 'encoder_q.blocks.0.norm1.bias', 'encoder_q.blocks.0.norm2.weight', 'encoder_q.blocks.0.norm2.bias', 'encoder_q.blocks.0.attn.qkv.weight', 'encoder_q.blocks.0.attn.proj.weight', 'encoder_q.blocks.0.attn.proj.bias', 'encoder_q.blocks.0.mlp.fc1.weight', 'encoder_q.blocks.0.mlp.fc1.bias', 'encoder_q.blocks.0.mlp.fc2.weight', 'encoder_q.blocks.0.mlp.fc2.bias', 'encoder_q.blocks.1.norm1.weight', 'encoder_q.blocks.1.norm1.bias', 'encoder_q.blocks.1.norm2.weight', 'encoder_q.blocks.1.norm2.bias', 'encoder_q.blocks.1.attn.qkv.weight', 'encoder_q.blocks.1.attn.proj.weight', 'encoder_q.blocks.1.attn.proj.bias', 'encoder_q.blocks.1.mlp.fc1.weight', 'encoder_q.blocks.1.mlp.fc1.bias', 'encoder_q.blocks.1.mlp.fc2.weight', 'encoder_q.blocks.1.mlp.fc2.bias', 'encoder_q.blocks.2.norm1.weight', 'encoder_q.blocks.2.norm1.bias', 'encoder_q.blocks.2.norm2.weight', 'encoder_q.blocks.2.norm2.bias', 'encoder_q.blocks.2.attn.qkv.weight', 'encoder_q.blocks.2.attn.proj.weight', 'encoder_q.blocks.2.attn.proj.bias', 'encoder_q.blocks.2.mlp.fc1.weight', 'encoder_q.blocks.2.mlp.fc1.bias', 'encoder_q.blocks.2.mlp.fc2.weight', 'encoder_q.blocks.2.mlp.fc2.bias', 'encoder_q.blocks.3.norm1.weight', 'encoder_q.blocks.3.norm1.bias', 'encoder_q.blocks.3.norm2.weight', 'encoder_q.blocks.3.norm2.bias', 'encoder_q.blocks.3.attn.qkv.weight', 'encoder_q.blocks.3.attn.proj.weight', 'encoder_q.blocks.3.attn.proj.bias', 'encoder_q.blocks.3.mlp.fc1.weight', 'encoder_q.blocks.3.mlp.fc1.bias', 'encoder_q.blocks.3.mlp.fc2.weight', 'encoder_q.blocks.3.mlp.fc2.bias', 'encoder_q.blocks.4.norm1.weight', 'encoder_q.blocks.4.norm1.bias', 'encoder_q.blocks.4.norm2.weight', 'encoder_q.blocks.4.norm2.bias', 'encoder_q.blocks.4.attn.qkv.weight', 'encoder_q.blocks.4.attn.proj.weight', 'encoder_q.blocks.4.attn.proj.bias', 'encoder_q.blocks.4.mlp.fc1.weight', 'encoder_q.blocks.4.mlp.fc1.bias', 'encoder_q.blocks.4.mlp.fc2.weight', 'encoder_q.blocks.4.mlp.fc2.bias', 'encoder_q.blocks.5.norm1.weight', 'encoder_q.blocks.5.norm1.bias', 'encoder_q.blocks.5.norm2.weight', 'encoder_q.blocks.5.norm2.bias', 'encoder_q.blocks.5.attn.qkv.weight', 'encoder_q.blocks.5.attn.proj.weight', 'encoder_q.blocks.5.attn.proj.bias', 'encoder_q.blocks.5.mlp.fc1.weight', 'encoder_q.blocks.5.mlp.fc1.bias', 'encoder_q.blocks.5.mlp.fc2.weight', 'encoder_q.blocks.5.mlp.fc2.bias', 'encoder_q.blocks.6.norm1.weight', 'encoder_q.blocks.6.norm1.bias', 'encoder_q.blocks.6.norm2.weight', 'encoder_q.blocks.6.norm2.bias', 'encoder_q.blocks.6.attn.qkv.weight', 'encoder_q.blocks.6.attn.proj.weight', 'encoder_q.blocks.6.attn.proj.bias', 'encoder_q.blocks.6.mlp.fc1.weight', 'encoder_q.blocks.6.mlp.fc1.bias', 'encoder_q.blocks.6.mlp.fc2.weight', 'encoder_q.blocks.6.mlp.fc2.bias', 'encoder_q.blocks.7.norm1.weight', 'encoder_q.blocks.7.norm1.bias', 'encoder_q.blocks.7.norm2.weight', 'encoder_q.blocks.7.norm2.bias', 'encoder_q.blocks.7.attn.qkv.weight', 'encoder_q.blocks.7.attn.proj.weight', 'encoder_q.blocks.7.attn.proj.bias', 'encoder_q.blocks.7.mlp.fc1.weight', 'encoder_q.blocks.7.mlp.fc1.bias', 'encoder_q.blocks.7.mlp.fc2.weight', 'encoder_q.blocks.7.mlp.fc2.bias', 'encoder_q.blocks.8.norm1.weight', 'encoder_q.blocks.8.norm1.bias', 'encoder_q.blocks.8.norm2.weight', 'encoder_q.blocks.8.norm2.bias', 'encoder_q.blocks.8.attn.qkv.weight', 'encoder_q.blocks.8.attn.proj.weight', 'encoder_q.blocks.8.attn.proj.bias', 'encoder_q.blocks.8.mlp.fc1.weight', 'encoder_q.blocks.8.mlp.fc1.bias', 'encoder_q.blocks.8.mlp.fc2.weight', 'encoder_q.blocks.8.mlp.fc2.bias', 'encoder_q.blocks.9.norm1.weight', 'encoder_q.blocks.9.norm1.bias', 'encoder_q.blocks.9.norm2.weight', 'encoder_q.blocks.9.norm2.bias', 'encoder_q.blocks.9.attn.qkv.weight', 'encoder_q.blocks.9.attn.proj.weight', 'encoder_q.blocks.9.attn.proj.bias', 'encoder_q.blocks.9.mlp.fc1.weight', 'encoder_q.blocks.9.mlp.fc1.bias', 'encoder_q.blocks.9.mlp.fc2.weight', 'encoder_q.blocks.9.mlp.fc2.bias', 'encoder_q.blocks.10.norm1.weight', 'encoder_q.blocks.10.norm1.bias', 'encoder_q.blocks.10.norm2.weight', 'encoder_q.blocks.10.norm2.bias', 'encoder_q.blocks.10.attn.qkv.weight', 'encoder_q.blocks.10.attn.proj.weight', 'encoder_q.blocks.10.attn.proj.bias', 'encoder_q.blocks.10.mlp.fc1.weight', 'encoder_q.blocks.10.mlp.fc1.bias', 'encoder_q.blocks.10.mlp.fc2.weight', 'encoder_q.blocks.10.mlp.fc2.bias', 'encoder_q.blocks.11.norm1.weight', 'encoder_q.blocks.11.norm1.bias', 'encoder_q.blocks.11.norm2.weight', 'encoder_q.blocks.11.norm2.bias', 'encoder_q.blocks.11.attn.qkv.weight', 'encoder_q.blocks.11.attn.proj.weight', 'encoder_q.blocks.11.attn.proj.bias', 'encoder_q.blocks.11.mlp.fc1.weight', 'encoder_q.blocks.11.mlp.fc1.bias', 'encoder_q.blocks.11.mlp.fc2.weight', 'encoder_q.blocks.11.mlp.fc2.bias', 'encoder_q.norm.weight', 'encoder_q.norm.bias', 'encoder_q.feature.0.weight', 'encoder_q.feature.1.weight', 'encoder_q.feature.1.bias', 'encoder_q.feature.1.running_mean', 'encoder_q.feature.1.running_var', 'encoder_q.feature.1.num_batches_tracked', 'encoder_q.feature.2.weight', 'encoder_q.feature.3.weight', 'encoder_q.feature.3.bias', 'encoder_q.feature.3.running_mean', 'encoder_q.feature.3.running_var', 'encoder_q.feature.3.num_batches_tracked', 'encoder_k.pos_embed', 'encoder_k.patch_embed.proj.weight', 'encoder_k.patch_embed.proj.bias', 'encoder_k.blocks.0.norm1.weight', 'encoder_k.blocks.0.norm1.bias', 'encoder_k.blocks.0.norm2.weight', 'encoder_k.blocks.0.norm2.bias', 'encoder_k.blocks.0.attn.qkv.weight', 'encoder_k.blocks.0.attn.proj.weight', 'encoder_k.blocks.0.attn.proj.bias', 'encoder_k.blocks.0.mlp.fc1.weight', 'encoder_k.blocks.0.mlp.fc1.bias', 'encoder_k.blocks.0.mlp.fc2.weight', 'encoder_k.blocks.0.mlp.fc2.bias', 'encoder_k.blocks.1.norm1.weight', 'encoder_k.blocks.1.norm1.bias', 'encoder_k.blocks.1.norm2.weight', 'encoder_k.blocks.1.norm2.bias', 'encoder_k.blocks.1.attn.qkv.weight', 'encoder_k.blocks.1.attn.proj.weight', 'encoder_k.blocks.1.attn.proj.bias', 'encoder_k.blocks.1.mlp.fc1.weight', 'encoder_k.blocks.1.mlp.fc1.bias', 'encoder_k.blocks.1.mlp.fc2.weight', 'encoder_k.blocks.1.mlp.fc2.bias', 'encoder_k.blocks.2.norm1.weight', 'encoder_k.blocks.2.norm1.bias', 'encoder_k.blocks.2.norm2.weight', 'encoder_k.blocks.2.norm2.bias', 'encoder_k.blocks.2.attn.qkv.weight', 'encoder_k.blocks.2.attn.proj.weight', 'encoder_k.blocks.2.attn.proj.bias', 'encoder_k.blocks.2.mlp.fc1.weight', 'encoder_k.blocks.2.mlp.fc1.bias', 'encoder_k.blocks.2.mlp.fc2.weight', 'encoder_k.blocks.2.mlp.fc2.bias', 'encoder_k.blocks.3.norm1.weight', 'encoder_k.blocks.3.norm1.bias', 'encoder_k.blocks.3.norm2.weight', 'encoder_k.blocks.3.norm2.bias', 'encoder_k.blocks.3.attn.qkv.weight', 'encoder_k.blocks.3.attn.proj.weight', 'encoder_k.blocks.3.attn.proj.bias', 'encoder_k.blocks.3.mlp.fc1.weight', 'encoder_k.blocks.3.mlp.fc1.bias', 'encoder_k.blocks.3.mlp.fc2.weight', 'encoder_k.blocks.3.mlp.fc2.bias', 'encoder_k.blocks.4.norm1.weight', 'encoder_k.blocks.4.norm1.bias', 'encoder_k.blocks.4.norm2.weight', 'encoder_k.blocks.4.norm2.bias', 'encoder_k.blocks.4.attn.qkv.weight', 'encoder_k.blocks.4.attn.proj.weight', 'encoder_k.blocks.4.attn.proj.bias', 'encoder_k.blocks.4.mlp.fc1.weight', 'encoder_k.blocks.4.mlp.fc1.bias', 'encoder_k.blocks.4.mlp.fc2.weight', 'encoder_k.blocks.4.mlp.fc2.bias', 'encoder_k.blocks.5.norm1.weight', 'encoder_k.blocks.5.norm1.bias', 'encoder_k.blocks.5.norm2.weight', 'encoder_k.blocks.5.norm2.bias', 'encoder_k.blocks.5.attn.qkv.weight', 'encoder_k.blocks.5.attn.proj.weight', 'encoder_k.blocks.5.attn.proj.bias', 'encoder_k.blocks.5.mlp.fc1.weight', 'encoder_k.blocks.5.mlp.fc1.bias', 'encoder_k.blocks.5.mlp.fc2.weight', 'encoder_k.blocks.5.mlp.fc2.bias', 'encoder_k.blocks.6.norm1.weight', 'encoder_k.blocks.6.norm1.bias', 'encoder_k.blocks.6.norm2.weight', 'encoder_k.blocks.6.norm2.bias', 'encoder_k.blocks.6.attn.qkv.weight', 'encoder_k.blocks.6.attn.proj.weight', 'encoder_k.blocks.6.attn.proj.bias', 'encoder_k.blocks.6.mlp.fc1.weight', 'encoder_k.blocks.6.mlp.fc1.bias', 'encoder_k.blocks.6.mlp.fc2.weight', 'encoder_k.blocks.6.mlp.fc2.bias', 'encoder_k.blocks.7.norm1.weight', 'encoder_k.blocks.7.norm1.bias', 'encoder_k.blocks.7.norm2.weight', 'encoder_k.blocks.7.norm2.bias', 'encoder_k.blocks.7.attn.qkv.weight', 'encoder_k.blocks.7.attn.proj.weight', 'encoder_k.blocks.7.attn.proj.bias', 'encoder_k.blocks.7.mlp.fc1.weight', 'encoder_k.blocks.7.mlp.fc1.bias', 'encoder_k.blocks.7.mlp.fc2.weight', 'encoder_k.blocks.7.mlp.fc2.bias', 'encoder_k.blocks.8.norm1.weight', 'encoder_k.blocks.8.norm1.bias', 'encoder_k.blocks.8.norm2.weight', 'encoder_k.blocks.8.norm2.bias', 'encoder_k.blocks.8.attn.qkv.weight', 'encoder_k.blocks.8.attn.proj.weight', 'encoder_k.blocks.8.attn.proj.bias', 'encoder_k.blocks.8.mlp.fc1.weight', 'encoder_k.blocks.8.mlp.fc1.bias', 'encoder_k.blocks.8.mlp.fc2.weight', 'encoder_k.blocks.8.mlp.fc2.bias', 'encoder_k.blocks.9.norm1.weight', 'encoder_k.blocks.9.norm1.bias', 'encoder_k.blocks.9.norm2.weight', 'encoder_k.blocks.9.norm2.bias', 'encoder_k.blocks.9.attn.qkv.weight', 'encoder_k.blocks.9.attn.proj.weight', 'encoder_k.blocks.9.attn.proj.bias', 'encoder_k.blocks.9.mlp.fc1.weight', 'encoder_k.blocks.9.mlp.fc1.bias', 'encoder_k.blocks.9.mlp.fc2.weight', 'encoder_k.blocks.9.mlp.fc2.bias', 'encoder_k.blocks.10.norm1.weight', 'encoder_k.blocks.10.norm1.bias', 'encoder_k.blocks.10.norm2.weight', 'encoder_k.blocks.10.norm2.bias', 'encoder_k.blocks.10.attn.qkv.weight', 'encoder_k.blocks.10.attn.proj.weight', 'encoder_k.blocks.10.attn.proj.bias', 'encoder_k.blocks.10.mlp.fc1.weight', 'encoder_k.blocks.10.mlp.fc1.bias', 'encoder_k.blocks.10.mlp.fc2.weight', 'encoder_k.blocks.10.mlp.fc2.bias', 'encoder_k.blocks.11.norm1.weight', 'encoder_k.blocks.11.norm1.bias', 'encoder_k.blocks.11.norm2.weight', 'encoder_k.blocks.11.norm2.bias', 'encoder_k.blocks.11.attn.qkv.weight', 'encoder_k.blocks.11.attn.proj.weight', 'encoder_k.blocks.11.attn.proj.bias', 'encoder_k.blocks.11.mlp.fc1.weight', 'encoder_k.blocks.11.mlp.fc1.bias', 'encoder_k.blocks.11.mlp.fc2.weight', 'encoder_k.blocks.11.mlp.fc2.bias', 'encoder_k.norm.weight', 'encoder_k.norm.bias', 'encoder_k.feature.0.weight', 'encoder_k.feature.1.weight', 'encoder_k.feature.1.bias', 'encoder_k.feature.1.running_mean', 'encoder_k.feature.1.running_var', 'encoder_k.feature.1.num_batches_tracked', 'encoder_k.feature.2.weight', 'encoder_k.feature.3.weight', 'encoder_k.feature.3.bias', 'encoder_k.feature.3.running_mean', 'encoder_k.feature.3.running_var', 'encoder_k.feature.3.num_batches_tracked'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'norm.weight', 'norm.bias', 'feature.0.weight', 'feature.1.weight', 'feature.1.bias', 'feature.1.running_mean', 'feature.1.running_var', 'feature.1.num_batches_tracked', 'feature.2.weight', 'feature.3.weight', 'feature.3.bias', 'feature.3.running_mean', 'feature.3.running_var', 'feature.3.num_batches_tracked'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m unicom\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mViT-B/32\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(state_dict)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'state_dict' is not defined"
     ]
    }
   ],
   "source": [
    "model = unicom.load('ViT-B/32')[0]\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = torch.nn.Parameter(torch.ones(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicom.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unicom.load('ViT-B/32')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "avgpool = nn.AdaptiveAvgPool1d(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Block(\n",
       "  (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): Attention(\n",
       "    (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (drop_path): DropPath(drop_prob=0.100)\n",
       "  (mlp): Mlp(\n",
       "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (act): ReLU6()\n",
       "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[len(model.blocks) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2b291ab3ea90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gp.sc.cc.tohoku.ac.jp/duanct/miniconda3/envs/py2cuda118/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn((32, 3, 224, 224))\n",
    "with torch.no_grad():\n",
    "    x1 = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "x = model.patch_embed(inputs)\n",
    "x = x + model.pos_embed\n",
    "for block in model.blocks:\n",
    "    x = block(x)\n",
    "    # print(x.shape)\n",
    "x = model.norm(x)\n",
    "# print(x.shape)\n",
    "x = x.reshape(32, -1)\n",
    "x = model.feature(x)\n",
    "# print(x.shape)\n",
    "\n",
    "\n",
    "print(torch.allclose(x, x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.any(x == x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x == x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgpool(x.transpose(1, 2)).squeeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1             [2, 768, 7, 7]       2,360,064\n",
      "    PatchEmbedding-2               [2, 49, 768]               0\n",
      "         LayerNorm-3               [2, 49, 768]           1,536\n",
      "            Linear-4              [2, 49, 2304]       1,769,472\n",
      "            Linear-5               [2, 49, 768]         590,592\n",
      "         Attention-6               [2, 49, 768]               0\n",
      "          Identity-7               [2, 49, 768]               0\n",
      "         LayerNorm-8               [2, 49, 768]           1,536\n",
      "            Linear-9              [2, 49, 3072]       2,362,368\n",
      "            ReLU6-10              [2, 49, 3072]               0\n",
      "           Linear-11               [2, 49, 768]       2,360,064\n",
      "              Mlp-12               [2, 49, 768]               0\n",
      "         Identity-13               [2, 49, 768]               0\n",
      "            Block-14               [2, 49, 768]               0\n",
      "        LayerNorm-15               [2, 49, 768]           1,536\n",
      "           Linear-16              [2, 49, 2304]       1,769,472\n",
      "           Linear-17               [2, 49, 768]         590,592\n",
      "        Attention-18               [2, 49, 768]               0\n",
      "         DropPath-19               [2, 49, 768]               0\n",
      "        LayerNorm-20               [2, 49, 768]           1,536\n",
      "           Linear-21              [2, 49, 3072]       2,362,368\n",
      "            ReLU6-22              [2, 49, 3072]               0\n",
      "           Linear-23               [2, 49, 768]       2,360,064\n",
      "              Mlp-24               [2, 49, 768]               0\n",
      "         DropPath-25               [2, 49, 768]               0\n",
      "            Block-26               [2, 49, 768]               0\n",
      "        LayerNorm-27               [2, 49, 768]           1,536\n",
      "           Linear-28              [2, 49, 2304]       1,769,472\n",
      "           Linear-29               [2, 49, 768]         590,592\n",
      "        Attention-30               [2, 49, 768]               0\n",
      "         DropPath-31               [2, 49, 768]               0\n",
      "        LayerNorm-32               [2, 49, 768]           1,536\n",
      "           Linear-33              [2, 49, 3072]       2,362,368\n",
      "            ReLU6-34              [2, 49, 3072]               0\n",
      "           Linear-35               [2, 49, 768]       2,360,064\n",
      "              Mlp-36               [2, 49, 768]               0\n",
      "         DropPath-37               [2, 49, 768]               0\n",
      "            Block-38               [2, 49, 768]               0\n",
      "        LayerNorm-39               [2, 49, 768]           1,536\n",
      "           Linear-40              [2, 49, 2304]       1,769,472\n",
      "           Linear-41               [2, 49, 768]         590,592\n",
      "        Attention-42               [2, 49, 768]               0\n",
      "         DropPath-43               [2, 49, 768]               0\n",
      "        LayerNorm-44               [2, 49, 768]           1,536\n",
      "           Linear-45              [2, 49, 3072]       2,362,368\n",
      "            ReLU6-46              [2, 49, 3072]               0\n",
      "           Linear-47               [2, 49, 768]       2,360,064\n",
      "              Mlp-48               [2, 49, 768]               0\n",
      "         DropPath-49               [2, 49, 768]               0\n",
      "            Block-50               [2, 49, 768]               0\n",
      "        LayerNorm-51               [2, 49, 768]           1,536\n",
      "           Linear-52              [2, 49, 2304]       1,769,472\n",
      "           Linear-53               [2, 49, 768]         590,592\n",
      "        Attention-54               [2, 49, 768]               0\n",
      "         DropPath-55               [2, 49, 768]               0\n",
      "        LayerNorm-56               [2, 49, 768]           1,536\n",
      "           Linear-57              [2, 49, 3072]       2,362,368\n",
      "            ReLU6-58              [2, 49, 3072]               0\n",
      "           Linear-59               [2, 49, 768]       2,360,064\n",
      "              Mlp-60               [2, 49, 768]               0\n",
      "         DropPath-61               [2, 49, 768]               0\n",
      "            Block-62               [2, 49, 768]               0\n",
      "        LayerNorm-63               [2, 49, 768]           1,536\n",
      "           Linear-64              [2, 49, 2304]       1,769,472\n",
      "           Linear-65               [2, 49, 768]         590,592\n",
      "        Attention-66               [2, 49, 768]               0\n",
      "         DropPath-67               [2, 49, 768]               0\n",
      "        LayerNorm-68               [2, 49, 768]           1,536\n",
      "           Linear-69              [2, 49, 3072]       2,362,368\n",
      "            ReLU6-70              [2, 49, 3072]               0\n",
      "           Linear-71               [2, 49, 768]       2,360,064\n",
      "              Mlp-72               [2, 49, 768]               0\n",
      "         DropPath-73               [2, 49, 768]               0\n",
      "            Block-74               [2, 49, 768]               0\n",
      "        LayerNorm-75               [2, 49, 768]           1,536\n",
      "           Linear-76              [2, 49, 2304]       1,769,472\n",
      "           Linear-77               [2, 49, 768]         590,592\n",
      "        Attention-78               [2, 49, 768]               0\n",
      "         DropPath-79               [2, 49, 768]               0\n",
      "        LayerNorm-80               [2, 49, 768]           1,536\n",
      "           Linear-81              [2, 49, 3072]       2,362,368\n",
      "            ReLU6-82              [2, 49, 3072]               0\n",
      "           Linear-83               [2, 49, 768]       2,360,064\n",
      "              Mlp-84               [2, 49, 768]               0\n",
      "         DropPath-85               [2, 49, 768]               0\n",
      "            Block-86               [2, 49, 768]               0\n",
      "        LayerNorm-87               [2, 49, 768]           1,536\n",
      "           Linear-88              [2, 49, 2304]       1,769,472\n",
      "           Linear-89               [2, 49, 768]         590,592\n",
      "        Attention-90               [2, 49, 768]               0\n",
      "         DropPath-91               [2, 49, 768]               0\n",
      "        LayerNorm-92               [2, 49, 768]           1,536\n",
      "           Linear-93              [2, 49, 3072]       2,362,368\n",
      "            ReLU6-94              [2, 49, 3072]               0\n",
      "           Linear-95               [2, 49, 768]       2,360,064\n",
      "              Mlp-96               [2, 49, 768]               0\n",
      "         DropPath-97               [2, 49, 768]               0\n",
      "            Block-98               [2, 49, 768]               0\n",
      "        LayerNorm-99               [2, 49, 768]           1,536\n",
      "          Linear-100              [2, 49, 2304]       1,769,472\n",
      "          Linear-101               [2, 49, 768]         590,592\n",
      "       Attention-102               [2, 49, 768]               0\n",
      "        DropPath-103               [2, 49, 768]               0\n",
      "       LayerNorm-104               [2, 49, 768]           1,536\n",
      "          Linear-105              [2, 49, 3072]       2,362,368\n",
      "           ReLU6-106              [2, 49, 3072]               0\n",
      "          Linear-107               [2, 49, 768]       2,360,064\n",
      "             Mlp-108               [2, 49, 768]               0\n",
      "        DropPath-109               [2, 49, 768]               0\n",
      "           Block-110               [2, 49, 768]               0\n",
      "       LayerNorm-111               [2, 49, 768]           1,536\n",
      "          Linear-112              [2, 49, 2304]       1,769,472\n",
      "          Linear-113               [2, 49, 768]         590,592\n",
      "       Attention-114               [2, 49, 768]               0\n",
      "        DropPath-115               [2, 49, 768]               0\n",
      "       LayerNorm-116               [2, 49, 768]           1,536\n",
      "          Linear-117              [2, 49, 3072]       2,362,368\n",
      "           ReLU6-118              [2, 49, 3072]               0\n",
      "          Linear-119               [2, 49, 768]       2,360,064\n",
      "             Mlp-120               [2, 49, 768]               0\n",
      "        DropPath-121               [2, 49, 768]               0\n",
      "           Block-122               [2, 49, 768]               0\n",
      "       LayerNorm-123               [2, 49, 768]           1,536\n",
      "          Linear-124              [2, 49, 2304]       1,769,472\n",
      "          Linear-125               [2, 49, 768]         590,592\n",
      "       Attention-126               [2, 49, 768]               0\n",
      "        DropPath-127               [2, 49, 768]               0\n",
      "       LayerNorm-128               [2, 49, 768]           1,536\n",
      "          Linear-129              [2, 49, 3072]       2,362,368\n",
      "           ReLU6-130              [2, 49, 3072]               0\n",
      "          Linear-131               [2, 49, 768]       2,360,064\n",
      "             Mlp-132               [2, 49, 768]               0\n",
      "        DropPath-133               [2, 49, 768]               0\n",
      "           Block-134               [2, 49, 768]               0\n",
      "       LayerNorm-135               [2, 49, 768]           1,536\n",
      "          Linear-136              [2, 49, 2304]       1,769,472\n",
      "          Linear-137               [2, 49, 768]         590,592\n",
      "       Attention-138               [2, 49, 768]               0\n",
      "        DropPath-139               [2, 49, 768]               0\n",
      "       LayerNorm-140               [2, 49, 768]           1,536\n",
      "          Linear-141              [2, 49, 3072]       2,362,368\n",
      "           ReLU6-142              [2, 49, 3072]               0\n",
      "          Linear-143               [2, 49, 768]       2,360,064\n",
      "             Mlp-144               [2, 49, 768]               0\n",
      "        DropPath-145               [2, 49, 768]               0\n",
      "           Block-146               [2, 49, 768]               0\n",
      "       LayerNorm-147               [2, 49, 768]           1,536\n",
      "          Linear-148                   [2, 768]      28,901,376\n",
      "     BatchNorm1d-149                   [2, 768]           1,536\n",
      "          Linear-150                   [2, 512]         393,216\n",
      "     BatchNorm1d-151                   [2, 512]           1,024\n",
      "================================================================\n",
      "Total params: 116,685,568\n",
      "Trainable params: 116,685,568\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.15\n",
      "Forward/backward pass size (MB): 139.57\n",
      "Params size (MB): 445.12\n",
      "Estimated Total Size (MB): 585.84\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn((32, 3, 224, 224))\n",
    "model = unicom.load('ViT-B/32')[0]\n",
    "model = model.cuda()\n",
    "summary(model, (3, 224, 224), batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [2, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2          [2, 64, 112, 112]             128\n",
      "              ReLU-3          [2, 64, 112, 112]               0\n",
      "         MaxPool2d-4            [2, 64, 56, 56]               0\n",
      "            Conv2d-5            [2, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6            [2, 64, 56, 56]             128\n",
      "              ReLU-7            [2, 64, 56, 56]               0\n",
      "            Conv2d-8            [2, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9            [2, 64, 56, 56]             128\n",
      "             ReLU-10            [2, 64, 56, 56]               0\n",
      "           Conv2d-11           [2, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12           [2, 256, 56, 56]             512\n",
      "           Conv2d-13           [2, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14           [2, 256, 56, 56]             512\n",
      "             ReLU-15           [2, 256, 56, 56]               0\n",
      "       Bottleneck-16           [2, 256, 56, 56]               0\n",
      "           Conv2d-17            [2, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18            [2, 64, 56, 56]             128\n",
      "             ReLU-19            [2, 64, 56, 56]               0\n",
      "           Conv2d-20            [2, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21            [2, 64, 56, 56]             128\n",
      "             ReLU-22            [2, 64, 56, 56]               0\n",
      "           Conv2d-23           [2, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24           [2, 256, 56, 56]             512\n",
      "             ReLU-25           [2, 256, 56, 56]               0\n",
      "       Bottleneck-26           [2, 256, 56, 56]               0\n",
      "           Conv2d-27            [2, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28            [2, 64, 56, 56]             128\n",
      "             ReLU-29            [2, 64, 56, 56]               0\n",
      "           Conv2d-30            [2, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31            [2, 64, 56, 56]             128\n",
      "             ReLU-32            [2, 64, 56, 56]               0\n",
      "           Conv2d-33           [2, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34           [2, 256, 56, 56]             512\n",
      "             ReLU-35           [2, 256, 56, 56]               0\n",
      "       Bottleneck-36           [2, 256, 56, 56]               0\n",
      "           Conv2d-37           [2, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38           [2, 128, 56, 56]             256\n",
      "             ReLU-39           [2, 128, 56, 56]               0\n",
      "           Conv2d-40           [2, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41           [2, 128, 28, 28]             256\n",
      "             ReLU-42           [2, 128, 28, 28]               0\n",
      "           Conv2d-43           [2, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44           [2, 512, 28, 28]           1,024\n",
      "           Conv2d-45           [2, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46           [2, 512, 28, 28]           1,024\n",
      "             ReLU-47           [2, 512, 28, 28]               0\n",
      "       Bottleneck-48           [2, 512, 28, 28]               0\n",
      "           Conv2d-49           [2, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50           [2, 128, 28, 28]             256\n",
      "             ReLU-51           [2, 128, 28, 28]               0\n",
      "           Conv2d-52           [2, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53           [2, 128, 28, 28]             256\n",
      "             ReLU-54           [2, 128, 28, 28]               0\n",
      "           Conv2d-55           [2, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56           [2, 512, 28, 28]           1,024\n",
      "             ReLU-57           [2, 512, 28, 28]               0\n",
      "       Bottleneck-58           [2, 512, 28, 28]               0\n",
      "           Conv2d-59           [2, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60           [2, 128, 28, 28]             256\n",
      "             ReLU-61           [2, 128, 28, 28]               0\n",
      "           Conv2d-62           [2, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63           [2, 128, 28, 28]             256\n",
      "             ReLU-64           [2, 128, 28, 28]               0\n",
      "           Conv2d-65           [2, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66           [2, 512, 28, 28]           1,024\n",
      "             ReLU-67           [2, 512, 28, 28]               0\n",
      "       Bottleneck-68           [2, 512, 28, 28]               0\n",
      "           Conv2d-69           [2, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70           [2, 128, 28, 28]             256\n",
      "             ReLU-71           [2, 128, 28, 28]               0\n",
      "           Conv2d-72           [2, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73           [2, 128, 28, 28]             256\n",
      "             ReLU-74           [2, 128, 28, 28]               0\n",
      "           Conv2d-75           [2, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76           [2, 512, 28, 28]           1,024\n",
      "             ReLU-77           [2, 512, 28, 28]               0\n",
      "       Bottleneck-78           [2, 512, 28, 28]               0\n",
      "           Conv2d-79           [2, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80           [2, 256, 28, 28]             512\n",
      "             ReLU-81           [2, 256, 28, 28]               0\n",
      "           Conv2d-82           [2, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83           [2, 256, 14, 14]             512\n",
      "             ReLU-84           [2, 256, 14, 14]               0\n",
      "           Conv2d-85          [2, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86          [2, 1024, 14, 14]           2,048\n",
      "           Conv2d-87          [2, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88          [2, 1024, 14, 14]           2,048\n",
      "             ReLU-89          [2, 1024, 14, 14]               0\n",
      "       Bottleneck-90          [2, 1024, 14, 14]               0\n",
      "           Conv2d-91           [2, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92           [2, 256, 14, 14]             512\n",
      "             ReLU-93           [2, 256, 14, 14]               0\n",
      "           Conv2d-94           [2, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95           [2, 256, 14, 14]             512\n",
      "             ReLU-96           [2, 256, 14, 14]               0\n",
      "           Conv2d-97          [2, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98          [2, 1024, 14, 14]           2,048\n",
      "             ReLU-99          [2, 1024, 14, 14]               0\n",
      "      Bottleneck-100          [2, 1024, 14, 14]               0\n",
      "          Conv2d-101           [2, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102           [2, 256, 14, 14]             512\n",
      "            ReLU-103           [2, 256, 14, 14]               0\n",
      "          Conv2d-104           [2, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105           [2, 256, 14, 14]             512\n",
      "            ReLU-106           [2, 256, 14, 14]               0\n",
      "          Conv2d-107          [2, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108          [2, 1024, 14, 14]           2,048\n",
      "            ReLU-109          [2, 1024, 14, 14]               0\n",
      "      Bottleneck-110          [2, 1024, 14, 14]               0\n",
      "          Conv2d-111           [2, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112           [2, 256, 14, 14]             512\n",
      "            ReLU-113           [2, 256, 14, 14]               0\n",
      "          Conv2d-114           [2, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115           [2, 256, 14, 14]             512\n",
      "            ReLU-116           [2, 256, 14, 14]               0\n",
      "          Conv2d-117          [2, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118          [2, 1024, 14, 14]           2,048\n",
      "            ReLU-119          [2, 1024, 14, 14]               0\n",
      "      Bottleneck-120          [2, 1024, 14, 14]               0\n",
      "          Conv2d-121           [2, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122           [2, 256, 14, 14]             512\n",
      "            ReLU-123           [2, 256, 14, 14]               0\n",
      "          Conv2d-124           [2, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125           [2, 256, 14, 14]             512\n",
      "            ReLU-126           [2, 256, 14, 14]               0\n",
      "          Conv2d-127          [2, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128          [2, 1024, 14, 14]           2,048\n",
      "            ReLU-129          [2, 1024, 14, 14]               0\n",
      "      Bottleneck-130          [2, 1024, 14, 14]               0\n",
      "          Conv2d-131           [2, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132           [2, 256, 14, 14]             512\n",
      "            ReLU-133           [2, 256, 14, 14]               0\n",
      "          Conv2d-134           [2, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135           [2, 256, 14, 14]             512\n",
      "            ReLU-136           [2, 256, 14, 14]               0\n",
      "          Conv2d-137          [2, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138          [2, 1024, 14, 14]           2,048\n",
      "            ReLU-139          [2, 1024, 14, 14]               0\n",
      "      Bottleneck-140          [2, 1024, 14, 14]               0\n",
      "          Conv2d-141           [2, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142           [2, 512, 14, 14]           1,024\n",
      "            ReLU-143           [2, 512, 14, 14]               0\n",
      "          Conv2d-144             [2, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145             [2, 512, 7, 7]           1,024\n",
      "            ReLU-146             [2, 512, 7, 7]               0\n",
      "          Conv2d-147            [2, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148            [2, 2048, 7, 7]           4,096\n",
      "          Conv2d-149            [2, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150            [2, 2048, 7, 7]           4,096\n",
      "            ReLU-151            [2, 2048, 7, 7]               0\n",
      "      Bottleneck-152            [2, 2048, 7, 7]               0\n",
      "          Conv2d-153             [2, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154             [2, 512, 7, 7]           1,024\n",
      "            ReLU-155             [2, 512, 7, 7]               0\n",
      "          Conv2d-156             [2, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157             [2, 512, 7, 7]           1,024\n",
      "            ReLU-158             [2, 512, 7, 7]               0\n",
      "          Conv2d-159            [2, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160            [2, 2048, 7, 7]           4,096\n",
      "            ReLU-161            [2, 2048, 7, 7]               0\n",
      "      Bottleneck-162            [2, 2048, 7, 7]               0\n",
      "          Conv2d-163             [2, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164             [2, 512, 7, 7]           1,024\n",
      "            ReLU-165             [2, 512, 7, 7]               0\n",
      "          Conv2d-166             [2, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167             [2, 512, 7, 7]           1,024\n",
      "            ReLU-168             [2, 512, 7, 7]               0\n",
      "          Conv2d-169            [2, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170            [2, 2048, 7, 7]           4,096\n",
      "            ReLU-171            [2, 2048, 7, 7]               0\n",
      "      Bottleneck-172            [2, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173            [2, 2048, 1, 1]               0\n",
      "          Linear-174                  [2, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 25,557,032\n",
      "Trainable params: 25,557,032\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.15\n",
      "Forward/backward pass size (MB): 573.12\n",
      "Params size (MB): 97.49\n",
      "Estimated Total Size (MB): 671.76\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet50()\n",
    "model = model.cuda()\n",
    "summary(model, (3, 224, 224), batch_size=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py2cuda118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
