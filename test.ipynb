{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gp.sc.cc.tohoku.ac.jp/duanct/miniconda3/envs/py2cuda118/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from vit_model import MaskedAutoEncoderViT\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn((1, 1, 224, 224))\n",
    "mae = MaskedAutoEncoderViT(in_chans=1)\n",
    "patched_inputs = mae.patch_embed(inputs)\n",
    "print(patched_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 49, 768])\n",
      "torch.Size([1, 196])\n",
      "torch.Size([1, 196])\n"
     ]
    }
   ],
   "source": [
    "patched_inputs = patched_inputs + mae.pos_embed[:, 1:, :]\n",
    "masked_x, mask, ids_restore = mae.random_masking(patched_inputs, 0.75)\n",
    "print(masked_x.shape)\n",
    "print(mask.shape)\n",
    "print(ids_restore.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 96,  40, 161, 165,  72,  18,  47, 133,  31,  37,  82,  14, 135, 108,\n",
       "         120, 130,  93, 158, 183, 122,  94, 111,  64, 137, 187, 152, 171,  15,\n",
       "         136,   2, 162,  63, 144,   3, 173, 156, 103, 172,  62, 127,   9,  79,\n",
       "         188,  29, 141, 184,  56,  17,   0, 147, 180, 194, 160, 179,  97, 153,\n",
       "          60,  74,  71,  33, 125, 138,  44, 190, 124, 110,  92,   8, 192,  22,\n",
       "         149,  59,  12,  41, 182, 163,  48,  99,  88,  65, 170, 132,  32, 114,\n",
       "         113, 128,  80, 164, 131,  45,  57, 129,  58,  27,  83, 167,  16, 185,\n",
       "          50,  68,  28,  67,  52, 123,  34, 166, 177, 126,  73, 181,  87,  55,\n",
       "          81,  69,  42,  95,  23,  84, 104,   6,  13, 189, 155,  85,  19,  30,\n",
       "         142, 148,  38,  49,  10,  53,   7,   5, 112, 176, 118, 116,  36, 186,\n",
       "         134, 140, 115,  77, 146,  21,  91, 117, 109,  43,  76, 154, 143, 121,\n",
       "          61, 150,  86,   4, 159,  20,  75,  66, 168,  11,  39,   1, 178,  46,\n",
       "         193, 119,  24,  70,  35, 105,  54, 102, 174, 169,  98, 101, 107,  89,\n",
       "         175, 106, 157, 151, 145,  78, 139,  25,  51,  90, 191, 100,  26, 195]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = torch.tensor([0, 3, 5])\n",
    "temp = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "torch.gather(temp, dim=0, index=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "cls_token = mae.cls_token + mae.pos_embed[:, :1, :]\n",
    "cls_tokens = cls_token.expand(patched_inputs.shape[0], -1, -1)\n",
    "print(cls_token.shape)\n",
    "print(cls_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('Fine-tuning on NDI images', add_help=True)\n",
    "    parser.add_argument('--batch size', default=32, type=int, help='Batch size per GPU')\n",
    "    parser.add_argument('--epochs', default=50, type=int)\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--model', default='mae_vit_large_patch16', type=str, metavar='MODEL',\n",
    "                        help='Name of model to train')\n",
    "\n",
    "    parser.add_argument('--input_size', default=224, type=int,\n",
    "                        help='images input size')\n",
    "\n",
    "    parser.add_argument('--mask_ratio', default=0.75, type=float,\n",
    "                        help='Masking ratio (percentage of removed patches).')\n",
    "\n",
    "    parser.add_argument('--layer_scale_init_value', default=1e-6, type=float,\n",
    "                        help=\"Layer scale initial values\")\n",
    "\n",
    "    # Optimizer parameters\n",
    "    parser.add_argument('--weight_decay', type=float, default=1e-4,\n",
    "                        help='weight decay (default: 0.05)')\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=5e-3, metavar='LR',\n",
    "                        help='learning rate (absolute lr)')\n",
    "\n",
    "    parser.add_argument('--min_lr', type=float, default=0., metavar='LR',\n",
    "                        help='lower lr bound for cosine schedulers that hit 0')\n",
    "\n",
    "    # Dataset Parameters\n",
    "    parser.add_argument('--output_dir', default='./checkpoints/',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--log_dir', default='./logs/',\n",
    "                        help='path where to save the log')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=19981303, type=int)\n",
    "\n",
    "    # Wandb Parameters\n",
    "    parser.add_argument('--project', default='Test which ViT suits NDI images best', type=str,\n",
    "                        help=\"The name of the W&B project where you're sending the new run.\")\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "a = torch.randn((32, 768))\n",
    "b = torch.randn((32, 768))\n",
    "\n",
    "a = F.normalize(a, dim=1)\n",
    "b = F.normalize(b, dim=1)\n",
    "\n",
    "sim_mat = torch.matmul(a, b.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4728)\n"
     ]
    }
   ],
   "source": [
    "logpt1 = F.log_softmax(sim_mat, dim=-1)\n",
    "logpt1 = torch.diag(logpt1)\n",
    "loss1 = -logpt1.mean()\n",
    "print(loss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4728)\n"
     ]
    }
   ],
   "source": [
    "f_loss1 = F.cross_entropy(sim_mat, torch.arange(0, 32))\n",
    "print(f_loss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import AverageMeter, ProgressMeter\n",
    "\n",
    "batch_time = AverageMeter('Batch Time', ':6.3f')\n",
    "data_time = AverageMeter('Data Time', ':6.3f')\n",
    "train_loss = AverageMeter('Train Loss', ':.4e')\n",
    "train_acc_10 = AverageMeter('Train Acc@10', ':6.2f')\n",
    "val_loss = AverageMeter('Val Loss', ':.4e')\n",
    "val_acc_10 = AverageMeter('Val Acc@10', ':6.2f')\n",
    "val_acc_20 = AverageMeter('Val Acc@20', ':6.2f')\n",
    "val_acc_30 = AverageMeter('Val Acc@30', ':6.2f')\n",
    "\n",
    "train_progress = ProgressMeter(32, [batch_time, data_time, train_loss, train_acc_10],\n",
    "                                prefix=f'Training Progress\\tEpoch: [50]')\n",
    "val_progress = ProgressMeter(32, [val_loss, val_acc_10, val_acc_20, val_acc_30],\n",
    "                                prefix=f'Validation Progress\\tEpoch: [50]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_fmtstr': '[{:2d}/32]',\n",
       " 'meters': [<utils.AverageMeter at 0x2b61f7f00eb0>,\n",
       "  <utils.AverageMeter at 0x2b61f7f00520>,\n",
       "  <utils.AverageMeter at 0x2b61f7f00fa0>,\n",
       "  <utils.AverageMeter at 0x2b61f7f00610>],\n",
       " 'prefix': 'Training Progress\\tEpoch: [50]'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_progress.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Data Time', 'fmt': '6.3f', 'val': 0, 'avg': 0, 'sum': 0, 'count': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_progress.meters[1].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress\tEpoch: [50][10/32]\tBatch Time  0.000 ( 0.000)\tData Time  0.000 ( 0.000)\tTrain Loss 0.0000e+00 (0.0000e+00)\tTrain Acc@10   0.00 (  0.00)\n"
     ]
    }
   ],
   "source": [
    "train_progress.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['swinv2_base_window8_256',\n",
       " 'swinv2_base_window12_192_22k',\n",
       " 'swinv2_base_window12to16_192to256_22kft1k',\n",
       " 'swinv2_base_window12to24_192to384_22kft1k',\n",
       " 'swinv2_base_window16_256',\n",
       " 'swinv2_cr_base_224',\n",
       " 'swinv2_cr_base_384',\n",
       " 'swinv2_cr_base_ns_224',\n",
       " 'swinv2_cr_giant_224',\n",
       " 'swinv2_cr_giant_384',\n",
       " 'swinv2_cr_huge_224',\n",
       " 'swinv2_cr_huge_384',\n",
       " 'swinv2_cr_large_224',\n",
       " 'swinv2_cr_large_384',\n",
       " 'swinv2_cr_small_224',\n",
       " 'swinv2_cr_small_384',\n",
       " 'swinv2_cr_small_ns_224',\n",
       " 'swinv2_cr_tiny_224',\n",
       " 'swinv2_cr_tiny_384',\n",
       " 'swinv2_cr_tiny_ns_224',\n",
       " 'swinv2_large_window12_192_22k',\n",
       " 'swinv2_large_window12to16_192to256_22kft1k',\n",
       " 'swinv2_large_window12to24_192to384_22kft1k',\n",
       " 'swinv2_small_window8_256',\n",
       " 'swinv2_small_window16_256',\n",
       " 'swinv2_tiny_window8_256',\n",
       " 'swinv2_tiny_window16_256']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "\n",
    "timm.list_models('swinv2*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gp.sc.cc.tohoku.ac.jp/duanct/miniconda3/envs/py2cuda118/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1677950040433/work/aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformerV2(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0): BasicLayer(\n",
      "      (blocks): ModuleList(\n",
      "        (0): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=4, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): Identity()\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=4, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): DropPath(drop_prob=0.004)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): DropPath(drop_prob=0.004)\n",
      "        )\n",
      "      )\n",
      "      (downsample): PatchMerging(\n",
      "        (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicLayer(\n",
      "      (blocks): ModuleList(\n",
      "        (0): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=8, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): DropPath(drop_prob=0.009)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): DropPath(drop_prob=0.009)\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=8, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): DropPath(drop_prob=0.013)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): DropPath(drop_prob=0.013)\n",
      "        )\n",
      "      )\n",
      "      (downsample): PatchMerging(\n",
      "        (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicLayer(\n",
      "      (blocks): ModuleList(\n",
      "        (0): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): DropPath(drop_prob=0.017)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): DropPath(drop_prob=0.017)\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): DropPath(drop_prob=0.022)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): DropPath(drop_prob=0.022)\n",
      "        )\n",
      "        (2): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): DropPath(drop_prob=0.026)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): DropPath(drop_prob=0.026)\n",
      "        )\n",
      "        (3): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): DropPath(drop_prob=0.030)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): DropPath(drop_prob=0.030)\n",
      "        )\n",
      "        (4): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): DropPath(drop_prob=0.035)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): DropPath(drop_prob=0.035)\n",
      "        )\n",
      "        (5): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): DropPath(drop_prob=0.039)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): DropPath(drop_prob=0.039)\n",
      "        )\n",
      "        (6): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): DropPath(drop_prob=0.043)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): DropPath(drop_prob=0.043)\n",
      "        )\n",
      "        (7): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): DropPath(drop_prob=0.048)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): DropPath(drop_prob=0.048)\n",
      "        )\n",
      "        (8): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): DropPath(drop_prob=0.052)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): DropPath(drop_prob=0.052)\n",
      "        )\n",
      "        (9): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): DropPath(drop_prob=0.057)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): DropPath(drop_prob=0.057)\n",
      "        )\n",
      "        (10): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): DropPath(drop_prob=0.061)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): DropPath(drop_prob=0.061)\n",
      "        )\n",
      "        (11): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): DropPath(drop_prob=0.065)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): DropPath(drop_prob=0.065)\n",
      "        )\n",
      "        (12): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): DropPath(drop_prob=0.070)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): DropPath(drop_prob=0.070)\n",
      "        )\n",
      "        (13): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): DropPath(drop_prob=0.074)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): DropPath(drop_prob=0.074)\n",
      "        )\n",
      "        (14): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): DropPath(drop_prob=0.078)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): DropPath(drop_prob=0.078)\n",
      "        )\n",
      "        (15): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): DropPath(drop_prob=0.083)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): DropPath(drop_prob=0.083)\n",
      "        )\n",
      "        (16): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): DropPath(drop_prob=0.087)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): DropPath(drop_prob=0.087)\n",
      "        )\n",
      "        (17): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): DropPath(drop_prob=0.091)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): DropPath(drop_prob=0.091)\n",
      "        )\n",
      "      )\n",
      "      (downsample): PatchMerging(\n",
      "        (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicLayer(\n",
      "      (blocks): ModuleList(\n",
      "        (0): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=32, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): DropPath(drop_prob=0.096)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): DropPath(drop_prob=0.096)\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          (attn): WindowAttention(\n",
      "            (cpb_mlp): Sequential(\n",
      "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=512, out_features=32, bias=False)\n",
      "            )\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path1): DropPath(drop_prob=0.100)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (drop_path2): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "      (downsample): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): Linear(in_features=1024, out_features=21841, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model('swinv2_base_window12_192_22k', pretrained=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window12_192_22k.pth',\n",
       " 'num_classes': 21841,\n",
       " 'input_size': (3, 192, 192),\n",
       " 'pool_size': None,\n",
       " 'crop_pct': 0.9,\n",
       " 'interpolation': 'bicubic',\n",
       " 'fixed_input_size': True,\n",
       " 'mean': (0.485, 0.456, 0.406),\n",
       " 'std': (0.229, 0.224, 0.225),\n",
       " 'first_conv': 'patch_embed.proj',\n",
       " 'classifier': 'head',\n",
       " 'architecture': 'swinv2_base_window12_192_22k'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.default_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Input image height (200) doesn't match model (256).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m inputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn((\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m200\u001b[39m, \u001b[39m200\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m model(inputs)\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/py2cuda118/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/py2cuda118/lib/python3.9/site-packages/timm/models/swin_transformer_v2.py:615\u001b[0m, in \u001b[0;36mSwinTransformerV2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 615\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_features(x)\n\u001b[1;32m    616\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_head(x)\n\u001b[1;32m    617\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/py2cuda118/lib/python3.9/site-packages/timm/models/swin_transformer_v2.py:598\u001b[0m, in \u001b[0;36mSwinTransformerV2.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_features\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 598\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpatch_embed(x)\n\u001b[1;32m    599\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mabsolute_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    600\u001b[0m         x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mabsolute_pos_embed\n",
      "File \u001b[0;32m~/miniconda3/envs/py2cuda118/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/py2cuda118/lib/python3.9/site-packages/timm/models/layers/patch_embed.py:42\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     41\u001b[0m     B, C, H, W \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n\u001b[0;32m---> 42\u001b[0m     _assert(H \u001b[39m==\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimg_size[\u001b[39m0\u001b[39;49m], \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mInput image height (\u001b[39;49m\u001b[39m{\u001b[39;49;00mH\u001b[39m}\u001b[39;49;00m\u001b[39m) doesn\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mt match model (\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimg_size[\u001b[39m0\u001b[39;49m]\u001b[39m}\u001b[39;49;00m\u001b[39m).\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     43\u001b[0m     _assert(W \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_size[\u001b[39m1\u001b[39m], \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput image width (\u001b[39m\u001b[39m{\u001b[39;00mW\u001b[39m}\u001b[39;00m\u001b[39m) doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match model (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_size[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/py2cuda118/lib/python3.9/site-packages/torch/__init__.py:1209\u001b[0m, in \u001b[0;36m_assert\u001b[0;34m(condition, message)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(condition) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mTensor \u001b[39mand\u001b[39;00m has_torch_function((condition,)):\n\u001b[1;32m   1208\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(_assert, (condition,), condition, message)\n\u001b[0;32m-> 1209\u001b[0m \u001b[39massert\u001b[39;00m condition, message\n",
      "\u001b[0;31mAssertionError\u001b[0m: Input image height (200) doesn't match model (256)."
     ]
    }
   ],
   "source": [
    "inputs = torch.randn((1, 3, 200, 200))\n",
    "model(inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2304, 128])\n"
     ]
    }
   ],
   "source": [
    "from timm.models.swin_transformer import PatchEmbed\n",
    "\n",
    "\n",
    "inputs = torch.randn((1, 3, 192, 192))\n",
    "layer = PatchEmbed(img_size=192, patch_size=4, embed_dim=128, in_chans=3, norm_layer=torch.nn.LayerNorm)\n",
    "print(layer(inputs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 21841])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn((1, 3, 192, 192))\n",
    "print(model(inputs).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py2cuda118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
