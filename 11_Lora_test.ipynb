{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loralib as lora\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_on_NDI import *\n",
    "from config import Config\n",
    "\n",
    "cfg = Config()\n",
    "device = torch.device(cfg.device)\n",
    "set_all_seeds(cfg.seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'loralib.layers.Conv2d'>\n"
     ]
    }
   ],
   "source": [
    "def convert_layers_LoRA(model, convert_types=[nn.Linear, nn.Conv2d], rank_factor=10):\n",
    "    named_modules = dict(model.named_modules())\n",
    "    for name, module in find_modules(model=model, search_classes=convert_types):\n",
    "        name_parts = name.split('.')\n",
    "        if len(name_parts) > 1:\n",
    "            parent_module = '.'.join(name_parts[:-1])\n",
    "            parent = named_modules[parent_module]\n",
    "            name = name.split('.')[-1]\n",
    "        else:\n",
    "            parent = model\n",
    "        update_to_LoRA_layer(module, parent, name, rank_factor)\n",
    "    return model\n",
    "\n",
    "\n",
    "def update_to_LoRA_layer(module, parent, name, rank_factor=10):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        setattr(parent, name, lora.Linear(module.in_features, module.out_features,\n",
    "                                          r=min(module.in_features, module.out_features) // rank_factor))\n",
    "    elif isinstance(module, nn.Conv2d):\n",
    "        if type(module.kernel_size) is not int:\n",
    "            kernel_size = module.kernel_size[0]\n",
    "            assert all([dim == kernel_size for dim in module.kernel_size])\n",
    "        else:\n",
    "            kernel_size = module.kernel_size\n",
    "        setattr(parent, name, lora.Conv2d(module.in_channels, module.out_channels, kernel_size,\n",
    "                                          r=min(module.in_channels, module.out_channels) // 4, \n",
    "                                          stride=module.stride, padding=module.padding, \n",
    "                                          dilation=module.dilation, groups=module.groups))\n",
    "\n",
    "\n",
    "def find_modules(model, search_classes=[nn.Linear, nn.Conv2d], exclusion=[lora.LoRALayer]):\n",
    "    layers_to_replace = []\n",
    "    for name, module in model.named_modules():\n",
    "        for _class in exclusion:\n",
    "            if isinstance(module, _class):\n",
    "                continue\n",
    "        for _class in search_classes:\n",
    "            if isinstance(module, _class):\n",
    "                layers_to_replace.append((name, module))\n",
    "    return layers_to_replace\n",
    "\n",
    "def load_checkpoints(base_encoder, ckpt_path):\n",
    "    temp = torch.load(ckpt_path)['state_dict']\n",
    "    state_dict = {}\n",
    "    for k, v in temp.items():\n",
    "        if 'encoder_q' in k:\n",
    "            if 'fc' not in k:\n",
    "                state_dict['.'.join(k.split('.')[1:])] = v\n",
    "    base_encoder.load_state_dict(state_dict, strict=False)\n",
    "    return base_encoder\n",
    "\n",
    "def get_modified_resnet50():\n",
    "    model = torchvision.models.resnet50()\n",
    "    model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    origin_mlp_dim = model.fc.in_features\n",
    "    model.fc = nn.Linear(origin_mlp_dim, 512)\n",
    "    return model\n",
    "\n",
    "model = get_modified_resnet50()\n",
    "model = convert_layers_LoRA(model)\n",
    "print(type(model.layer1[0].conv1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_checkpoints(model, './checkpoints/ImageNet_ALL_CHECK_400_Epoch.pth')\n",
    "lora.mark_only_lora_as_trainable(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:39:33,712][2436042988.py][line:4][INFO] This training is to do Lora test\n",
      "[2023-04-27 16:39:34,336][jupyter.py][line:224][ERROR] Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mzombiesama18\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/import/gp-home.cal/duanct/ProjectFolder/NDI_image_processing/wandb/run-20230427_163935-tghxeedt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zombiesama18/Lora%20test/runs/tghxeedt' target=\"_blank\">fold 0</a></strong> to <a href='https://wandb.ai/zombiesama18/Lora%20test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zombiesama18/Lora%20test' target=\"_blank\">https://wandb.ai/zombiesama18/Lora%20test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zombiesama18/Lora%20test/runs/tghxeedt' target=\"_blank\">https://wandb.ai/zombiesama18/Lora%20test/runs/tghxeedt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gp.sc.cc.tohoku.ac.jp/duanct/miniconda3/envs/py2cuda118/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress\tEpoch: [1/50][0/5]\tBatch Time  7.655 ( 7.655)\tData Time  2.187 ( 2.187)\tTrain Loss 1.0622e+01 (1.0622e+01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:40:03,323][train_on_NDI.py][line:206][INFO] Epoch: [0/50], train loss 9.827414512634277, val loss 6.525238990783691, val acc @ 10 8.333333015441895, val acc @ 20 12.5, val acc @ 30 16.66666603088379lr [0.005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [1/50][0/1]\tVal Loss 6.5252e+00 (6.5252e+00)\tVal Acc@10   8.33 (  8.33)\tVal Acc@20  12.50 ( 12.50)\tVal Acc@30  16.67 ( 16.67)\n",
      "Training Progress\tEpoch: [2/50][0/5]\tBatch Time  2.452 ( 2.452)\tData Time  2.405 ( 2.405)\tTrain Loss 1.0296e+01 (1.0296e+01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:40:28,348][train_on_NDI.py][line:206][INFO] Epoch: [1/50], train loss 9.483790397644043, val loss 6.499153137207031, val acc @ 10 0.0, val acc @ 20 0.0, val acc @ 30 8.333333015441895lr [0.004995116152859972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [2/50][0/1]\tVal Loss 6.4992e+00 (6.4992e+00)\tVal Acc@10   0.00 (  0.00)\tVal Acc@20   0.00 (  0.00)\tVal Acc@30   8.33 (  8.33)\n",
      "Training Progress\tEpoch: [3/50][0/5]\tBatch Time  1.495 ( 1.495)\tData Time  1.422 ( 1.422)\tTrain Loss 8.2523e+00 (8.2523e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:40:36,137][train_on_NDI.py][line:206][INFO] Epoch: [2/50], train loss 9.552892303466797, val loss 7.684744834899902, val acc @ 10 8.333333015441895, val acc @ 20 16.66666603088379, val acc @ 30 20.833332061767578lr [0.004980483885753333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [3/50][0/1]\tVal Loss 7.6847e+00 (7.6847e+00)\tVal Acc@10   8.33 (  8.33)\tVal Acc@20  16.67 ( 16.67)\tVal Acc@30  20.83 ( 20.83)\n",
      "Training Progress\tEpoch: [4/50][0/5]\tBatch Time  1.715 ( 1.715)\tData Time  1.652 ( 1.652)\tTrain Loss 9.1132e+00 (9.1132e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:40:44,307][train_on_NDI.py][line:206][INFO] Epoch: [3/50], train loss 9.709912109375, val loss 7.749393463134766, val acc @ 10 12.5, val acc @ 20 20.833332061767578, val acc @ 30 29.16666603088379lr [0.004956160945553504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [4/50][0/1]\tVal Loss 7.7494e+00 (7.7494e+00)\tVal Acc@10  12.50 ( 12.50)\tVal Acc@20  20.83 ( 20.83)\tVal Acc@30  29.17 ( 29.17)\n",
      "Training Progress\tEpoch: [5/50][0/5]\tBatch Time  2.309 ( 2.309)\tData Time  2.230 ( 2.230)\tTrain Loss 1.0484e+01 (1.0484e+01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:40:55,146][train_on_NDI.py][line:206][INFO] Epoch: [4/50], train loss 9.506353759765625, val loss 7.6983795166015625, val acc @ 10 16.66666603088379, val acc @ 20 25.0, val acc @ 30 29.16666603088379lr [0.004922243323793361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [5/50][0/1]\tVal Loss 7.6984e+00 (7.6984e+00)\tVal Acc@10  16.67 ( 16.67)\tVal Acc@20  25.00 ( 25.00)\tVal Acc@30  29.17 ( 29.17)\n",
      "Training Progress\tEpoch: [6/50][0/5]\tBatch Time  1.914 ( 1.914)\tData Time  1.851 ( 1.851)\tTrain Loss 9.8999e+00 (9.8999e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:41:03,416][train_on_NDI.py][line:206][INFO] Epoch: [5/50], train loss 9.742763328552247, val loss 7.3000922203063965, val acc @ 10 12.5, val acc @ 20 16.66666603088379, val acc @ 30 25.0lr [0.004878864877830505]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [6/50][0/1]\tVal Loss 7.3001e+00 (7.3001e+00)\tVal Acc@10  12.50 ( 12.50)\tVal Acc@20  16.67 ( 16.67)\tVal Acc@30  25.00 ( 25.00)\n",
      "Training Progress\tEpoch: [7/50][0/5]\tBatch Time  1.844 ( 1.844)\tData Time  1.799 ( 1.799)\tTrain Loss 9.5272e+00 (9.5272e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:41:13,873][train_on_NDI.py][line:206][INFO] Epoch: [6/50], train loss 9.439537239074706, val loss 7.056839942932129, val acc @ 10 8.333333015441895, val acc @ 20 8.333333015441895, val acc @ 30 8.333333015441895lr [0.004826196802573421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [7/50][0/1]\tVal Loss 7.0568e+00 (7.0568e+00)\tVal Acc@10   8.33 (  8.33)\tVal Acc@20   8.33 (  8.33)\tVal Acc@30   8.33 (  8.33)\n",
      "Training Progress\tEpoch: [8/50][0/5]\tBatch Time  1.397 ( 1.397)\tData Time  1.353 ( 1.353)\tTrain Loss 8.7252e+00 (8.7252e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:41:20,355][train_on_NDI.py][line:206][INFO] Epoch: [7/50], train loss 9.162652206420898, val loss 6.913612365722656, val acc @ 10 0.0, val acc @ 20 4.166666507720947, val acc @ 30 4.166666507720947lr [0.004764446954853398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [8/50][0/1]\tVal Loss 6.9136e+00 (6.9136e+00)\tVal Acc@10   0.00 (  0.00)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30   4.17 (  4.17)\n",
      "Training Progress\tEpoch: [9/50][0/5]\tBatch Time  0.768 ( 0.768)\tData Time  0.723 ( 0.723)\tTrain Loss 8.5092e+00 (8.5092e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:41:21,558][train_on_NDI.py][line:206][INFO] Epoch: [8/50], train loss 9.13502368927002, val loss 6.673752784729004, val acc @ 10 0.0, val acc @ 20 0.0, val acc @ 30 0.0lr [0.004693859033108562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [9/50][0/1]\tVal Loss 6.6738e+00 (6.6738e+00)\tVal Acc@10   0.00 (  0.00)\tVal Acc@20   0.00 (  0.00)\tVal Acc@30   0.00 (  0.00)\n",
      "Training Progress\tEpoch: [10/50][0/5]\tBatch Time  0.082 ( 0.082)\tData Time  0.037 ( 0.037)\tTrain Loss 9.2445e+00 (9.2445e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:41:22,090][train_on_NDI.py][line:206][INFO] Epoch: [9/50], train loss 9.1728084564209, val loss 6.735196590423584, val acc @ 10 0.0, val acc @ 20 0.0, val acc @ 30 0.0lr [0.004614711615617487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [10/50][0/1]\tVal Loss 6.7352e+00 (6.7352e+00)\tVal Acc@10   0.00 (  0.00)\tVal Acc@20   0.00 (  0.00)\tVal Acc@30   0.00 (  0.00)\n",
      "Training Progress\tEpoch: [11/50][0/5]\tBatch Time  0.081 ( 0.081)\tData Time  0.037 ( 0.037)\tTrain Loss 8.6434e+00 (8.6434e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:41:22,601][train_on_NDI.py][line:206][INFO] Epoch: [10/50], train loss 9.171144485473633, val loss 6.918503761291504, val acc @ 10 0.0, val acc @ 20 0.0, val acc @ 30 0.0lr [0.0045273170610779945]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [11/50][0/1]\tVal Loss 6.9185e+00 (6.9185e+00)\tVal Acc@10   0.00 (  0.00)\tVal Acc@20   0.00 (  0.00)\tVal Acc@30   0.00 (  0.00)\n",
      "Training Progress\tEpoch: [12/50][0/5]\tBatch Time  0.082 ( 0.082)\tData Time  0.038 ( 0.038)\tTrain Loss 8.6169e+00 (8.6169e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:41:23,123][train_on_NDI.py][line:206][INFO] Epoch: [11/50], train loss 9.264674377441406, val loss 6.998212814331055, val acc @ 10 8.333333015441895, val acc @ 20 8.333333015441895, val acc @ 30 8.333333015441895lr [0.004432020275870078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [12/50][0/1]\tVal Loss 6.9982e+00 (6.9982e+00)\tVal Acc@10   8.33 (  8.33)\tVal Acc@20   8.33 (  8.33)\tVal Acc@30   8.33 (  8.33)\n",
      "Training Progress\tEpoch: [13/50][0/5]\tBatch Time  0.083 ( 0.083)\tData Time  0.039 ( 0.039)\tTrain Loss 8.8562e+00 (8.8562e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:41:23,644][train_on_NDI.py][line:206][INFO] Epoch: [12/50], train loss 9.27197551727295, val loss 7.038619041442871, val acc @ 10 4.166666507720947, val acc @ 20 12.5, val acc @ 30 29.16666603088379lr [0.0043291973528679925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [13/50][0/1]\tVal Loss 7.0386e+00 (7.0386e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20  12.50 ( 12.50)\tVal Acc@30  29.17 ( 29.17)\n",
      "Training Progress\tEpoch: [14/50][0/5]\tBatch Time  0.084 ( 0.084)\tData Time  0.040 ( 0.040)\tTrain Loss 9.5368e+00 (9.5368e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:41:24,172][train_on_NDI.py][line:206][INFO] Epoch: [13/50], train loss 9.347010040283203, val loss 7.401834487915039, val acc @ 10 8.333333015441895, val acc @ 20 16.66666603088379, val acc @ 30 25.0lr [0.004219254087173503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [14/50][0/1]\tVal Loss 7.4018e+00 (7.4018e+00)\tVal Acc@10   8.33 (  8.33)\tVal Acc@20  16.67 ( 16.67)\tVal Acc@30  25.00 ( 25.00)\n",
      "Training Progress\tEpoch: [15/50][0/5]\tBatch Time  0.082 ( 0.082)\tData Time  0.038 ( 0.038)\tTrain Loss 8.2997e+00 (8.2997e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:41:24,704][train_on_NDI.py][line:206][INFO] Epoch: [14/50], train loss 9.182724952697754, val loss 7.718694686889648, val acc @ 10 0.0, val acc @ 20 8.333333015441895, val acc @ 30 16.66666603088379lr [0.004102624374628006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [15/50][0/1]\tVal Loss 7.7187e+00 (7.7187e+00)\tVal Acc@10   0.00 (  0.00)\tVal Acc@20   8.33 (  8.33)\tVal Acc@30  16.67 ( 16.67)\n",
      "Training Progress\tEpoch: [16/50][0/5]\tBatch Time  0.082 ( 0.082)\tData Time  0.038 ( 0.038)\tTrain Loss 8.5660e+00 (8.5660e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:41:25,218][train_on_NDI.py][line:206][INFO] Epoch: [15/50], train loss 9.082856559753418, val loss 7.769704818725586, val acc @ 10 4.166666507720947, val acc @ 20 8.333333015441895, val acc @ 30 16.66666603088379lr [0.00397976849942387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [16/50][0/1]\tVal Loss 7.7697e+00 (7.7697e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   8.33 (  8.33)\tVal Acc@30  16.67 ( 16.67)\n",
      "Training Progress\tEpoch: [17/50][0/5]\tBatch Time  0.104 ( 0.104)\tData Time  0.059 ( 0.059)\tTrain Loss 9.2193e+00 (9.2193e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:41:25,751][train_on_NDI.py][line:206][INFO] Epoch: [16/50], train loss 9.107094001770019, val loss 7.876561164855957, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 8.333333015441895lr [0.0038511713175730156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [17/50][0/1]\tVal Loss 7.8766e+00 (7.8766e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30   8.33 (  8.33)\n",
      "Training Progress\tEpoch: [18/50][0/5]\tBatch Time  0.082 ( 0.082)\tData Time  0.038 ( 0.038)\tTrain Loss 9.5723e+00 (9.5723e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:41:26,273][train_on_NDI.py][line:206][INFO] Epoch: [17/50], train loss 9.08580493927002, val loss 7.942053318023682, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 12.5lr [0.0037173403434017445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [18/50][0/1]\tVal Loss 7.9421e+00 (7.9421e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30  12.50 ( 12.50)\n",
      "Training Progress\tEpoch: [19/50][0/5]\tBatch Time  0.082 ( 0.082)\tData Time  0.038 ( 0.038)\tTrain Loss 1.0207e+01 (1.0207e+01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:41:26,811][train_on_NDI.py][line:206][INFO] Epoch: [18/50], train loss 9.360179901123047, val loss 8.004520416259766, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 12.5lr [0.003578803746623554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [19/50][0/1]\tVal Loss 8.0045e+00 (8.0045e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30  12.50 ( 12.50)\n",
      "Training Progress\tEpoch: [20/50][0/5]\tBatch Time  0.094 ( 0.094)\tData Time  0.050 ( 0.050)\tTrain Loss 8.3672e+00 (8.3672e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:41:27,359][train_on_NDI.py][line:206][INFO] Epoch: [19/50], train loss 8.874241065979003, val loss 8.000448226928711, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 8.333333015441895lr [0.0034361082678945774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [20/50][0/1]\tVal Loss 8.0004e+00 (8.0004e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30   8.33 (  8.33)\n",
      "Training Progress\tEpoch: [21/50][0/5]\tBatch Time  0.099 ( 0.099)\tData Time  0.054 ( 0.054)\tTrain Loss 9.3624e+00 (9.3624e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:41:28,440][train_on_NDI.py][line:206][INFO] Epoch: [20/50], train loss 9.041056442260743, val loss 8.005352020263672, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 12.5lr [0.003289817061077994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [21/50][0/1]\tVal Loss 8.0054e+00 (8.0054e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30  12.50 ( 12.50)\n",
      "Training Progress\tEpoch: [22/50][0/5]\tBatch Time  2.813 ( 2.813)\tData Time  2.729 ( 2.729)\tTrain Loss 8.9907e+00 (8.9907e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:41:40,410][train_on_NDI.py][line:206][INFO] Epoch: [21/50], train loss 9.192412376403809, val loss 7.9844560623168945, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 16.66666603088379lr [0.003140507470733014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [22/50][0/1]\tVal Loss 7.9845e+00 (7.9845e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30  16.67 ( 16.67)\n",
      "Training Progress\tEpoch: [23/50][0/5]\tBatch Time  2.080 ( 2.080)\tData Time  1.997 ( 1.997)\tTrain Loss 9.1410e+00 (9.1410e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:41:52,938][train_on_NDI.py][line:206][INFO] Epoch: [22/50], train loss 9.167637634277344, val loss 8.00990104675293, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 12.5lr [0.0029887687535996676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [23/50][0/1]\tVal Loss 8.0099e+00 (8.0099e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30  12.50 ( 12.50)\n",
      "Training Progress\tEpoch: [24/50][0/5]\tBatch Time  2.465 ( 2.465)\tData Time  2.381 ( 2.381)\tTrain Loss 9.2858e+00 (9.2858e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:42:05,878][train_on_NDI.py][line:206][INFO] Epoch: [23/50], train loss 9.031707382202148, val loss 7.968020439147949, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 12.5lr [0.0028351997530716524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [24/50][0/1]\tVal Loss 7.9680e+00 (7.9680e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30  12.50 ( 12.50)\n",
      "Training Progress\tEpoch: [25/50][0/5]\tBatch Time  1.867 ( 1.867)\tData Time  1.783 ( 1.783)\tTrain Loss 8.5122e+00 (8.5122e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:42:17,658][train_on_NDI.py][line:206][INFO] Epoch: [24/50], train loss 8.834493637084961, val loss 7.938023090362549, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 12.5lr [0.0026804065358350497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [25/50][0/1]\tVal Loss 7.9380e+00 (7.9380e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30  12.50 ( 12.50)\n",
      "Training Progress\tEpoch: [26/50][0/5]\tBatch Time  2.793 ( 2.793)\tData Time  2.723 ( 2.723)\tTrain Loss 9.1440e+00 (9.1440e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:42:29,206][train_on_NDI.py][line:206][INFO] Epoch: [25/50], train loss 8.79273796081543, val loss 7.9618964195251465, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 12.5lr [0.002524999999999999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [26/50][0/1]\tVal Loss 7.9619e+00 (7.9619e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30  12.50 ( 12.50)\n",
      "Training Progress\tEpoch: [27/50][0/5]\tBatch Time  1.390 ( 1.390)\tData Time  1.304 ( 1.304)\tTrain Loss 8.0331e+00 (8.0331e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:42:40,166][train_on_NDI.py][line:206][INFO] Epoch: [26/50], train loss 9.112697982788086, val loss 7.961707592010498, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 8.333333015441895lr [0.0023695934641649484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [27/50][0/1]\tVal Loss 7.9617e+00 (7.9617e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30   8.33 (  8.33)\n",
      "Training Progress\tEpoch: [28/50][0/5]\tBatch Time  2.191 ( 2.191)\tData Time  2.105 ( 2.105)\tTrain Loss 9.1215e+00 (9.1215e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:42:50,430][train_on_NDI.py][line:206][INFO] Epoch: [27/50], train loss 9.099729537963867, val loss 8.004538536071777, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 8.333333015441895lr [0.002214800246928346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [28/50][0/1]\tVal Loss 8.0045e+00 (8.0045e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30   8.33 (  8.33)\n",
      "Training Progress\tEpoch: [29/50][0/5]\tBatch Time  2.536 ( 2.536)\tData Time  2.484 ( 2.484)\tTrain Loss 7.9595e+00 (7.9595e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:43:00,663][train_on_NDI.py][line:206][INFO] Epoch: [28/50], train loss 9.022696495056152, val loss 8.029129981994629, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 8.333333015441895lr [0.002061231246400331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [29/50][0/1]\tVal Loss 8.0291e+00 (8.0291e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30   8.33 (  8.33)\n",
      "Training Progress\tEpoch: [30/50][0/5]\tBatch Time  2.549 ( 2.549)\tData Time  2.473 ( 2.473)\tTrain Loss 8.6182e+00 (8.6182e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:43:13,026][train_on_NDI.py][line:206][INFO] Epoch: [29/50], train loss 9.101908111572266, val loss 8.018778800964355, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 12.5lr [0.0019094925292669834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [30/50][0/1]\tVal Loss 8.0188e+00 (8.0188e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30  12.50 ( 12.50)\n",
      "Training Progress\tEpoch: [31/50][0/5]\tBatch Time  2.307 ( 2.307)\tData Time  2.224 ( 2.224)\tTrain Loss 9.0997e+00 (9.0997e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:43:26,751][train_on_NDI.py][line:206][INFO] Epoch: [30/50], train loss 8.754183197021485, val loss 8.022107124328613, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 12.5lr [0.0017601829389220051]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [31/50][0/1]\tVal Loss 8.0221e+00 (8.0221e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30  12.50 ( 12.50)\n",
      "Training Progress\tEpoch: [32/50][0/5]\tBatch Time  1.091 ( 1.091)\tData Time  1.043 ( 1.043)\tTrain Loss 8.4321e+00 (8.4321e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:43:36,139][train_on_NDI.py][line:206][INFO] Epoch: [31/50], train loss 8.873249530792236, val loss 8.000550270080566, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 12.5lr [0.001613891732105422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [32/50][0/1]\tVal Loss 8.0006e+00 (8.0006e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30  12.50 ( 12.50)\n",
      "Training Progress\tEpoch: [33/50][0/5]\tBatch Time  2.637 ( 2.637)\tData Time  2.554 ( 2.554)\tTrain Loss 9.5237e+00 (9.5237e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:43:49,205][train_on_NDI.py][line:206][INFO] Epoch: [32/50], train loss 8.912479591369628, val loss 7.91909122467041, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 8.333333015441895lr [0.0014711962533764443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [33/50][0/1]\tVal Loss 7.9191e+00 (7.9191e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30   8.33 (  8.33)\n",
      "Training Progress\tEpoch: [34/50][0/5]\tBatch Time  1.398 ( 1.398)\tData Time  1.314 ( 1.314)\tTrain Loss 9.6303e+00 (9.6303e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:44:00,587][train_on_NDI.py][line:206][INFO] Epoch: [33/50], train loss 8.673830604553222, val loss 7.90278434753418, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 12.5lr [0.0013326596565982536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [34/50][0/1]\tVal Loss 7.9028e+00 (7.9028e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30  12.50 ( 12.50)\n",
      "Training Progress\tEpoch: [35/50][0/5]\tBatch Time  1.508 ( 1.508)\tData Time  1.421 ( 1.421)\tTrain Loss 9.0882e+00 (9.0882e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:44:11,194][train_on_NDI.py][line:206][INFO] Epoch: [34/50], train loss 8.78334674835205, val loss 7.901920795440674, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 8.333333015441895lr [0.001198828682426982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [35/50][0/1]\tVal Loss 7.9019e+00 (7.9019e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30   8.33 (  8.33)\n",
      "Training Progress\tEpoch: [36/50][0/5]\tBatch Time  2.250 ( 2.250)\tData Time  2.163 ( 2.163)\tTrain Loss 9.3398e+00 (9.3398e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:44:22,676][train_on_NDI.py][line:206][INFO] Epoch: [35/50], train loss 8.587420845031739, val loss 7.925889015197754, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 8.333333015441895lr [0.0010702315005761286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [36/50][0/1]\tVal Loss 7.9259e+00 (7.9259e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30   8.33 (  8.33)\n",
      "Training Progress\tEpoch: [37/50][0/5]\tBatch Time  2.616 ( 2.616)\tData Time  2.544 ( 2.544)\tTrain Loss 8.2358e+00 (8.2358e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:44:35,652][train_on_NDI.py][line:206][INFO] Epoch: [36/50], train loss 8.72463264465332, val loss 7.9525346755981445, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 8.333333015441895lr [0.0009473756253719923]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [37/50][0/1]\tVal Loss 7.9525e+00 (7.9525e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30   8.33 (  8.33)\n",
      "Training Progress\tEpoch: [38/50][0/5]\tBatch Time  2.895 ( 2.895)\tData Time  2.811 ( 2.811)\tTrain Loss 8.6015e+00 (8.6015e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:44:46,674][train_on_NDI.py][line:206][INFO] Epoch: [37/50], train loss 8.877704238891601, val loss 7.934378623962402, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 8.333333015441895lr [0.000830745912826495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [38/50][0/1]\tVal Loss 7.9344e+00 (7.9344e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30   8.33 (  8.33)\n",
      "Training Progress\tEpoch: [39/50][0/5]\tBatch Time  2.042 ( 2.042)\tData Time  1.960 ( 1.960)\tTrain Loss 9.0125e+00 (9.0125e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:44:58,092][train_on_NDI.py][line:206][INFO] Epoch: [38/50], train loss 8.856092071533203, val loss 7.899031639099121, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 8.333333015441895lr [0.0007208026471320066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [39/50][0/1]\tVal Loss 7.8990e+00 (7.8990e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30   8.33 (  8.33)\n",
      "Training Progress\tEpoch: [40/50][0/5]\tBatch Time  2.200 ( 2.200)\tData Time  2.117 ( 2.117)\tTrain Loss 8.5082e+00 (8.5082e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:45:09,008][train_on_NDI.py][line:206][INFO] Epoch: [39/50], train loss 8.778559303283691, val loss 7.936297416687012, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 12.5lr [0.0006179797241299216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [40/50][0/1]\tVal Loss 7.9363e+00 (7.9363e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30  12.50 ( 12.50)\n",
      "Training Progress\tEpoch: [41/50][0/5]\tBatch Time  2.048 ( 2.048)\tData Time  1.995 ( 1.995)\tTrain Loss 8.8603e+00 (8.8603e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:45:20,203][train_on_NDI.py][line:206][INFO] Epoch: [40/50], train loss 8.640390586853027, val loss 7.87362003326416, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 12.5lr [0.000522682938922005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [41/50][0/1]\tVal Loss 7.8736e+00 (7.8736e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30  12.50 ( 12.50)\n",
      "Training Progress\tEpoch: [42/50][0/5]\tBatch Time  2.250 ( 2.250)\tData Time  2.197 ( 2.197)\tTrain Loss 8.5108e+00 (8.5108e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:45:33,118][train_on_NDI.py][line:206][INFO] Epoch: [41/50], train loss 8.822482490539551, val loss 7.915701389312744, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 8.333333015441895lr [0.000435288384382513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [42/50][0/1]\tVal Loss 7.9157e+00 (7.9157e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30   8.33 (  8.33)\n",
      "Training Progress\tEpoch: [43/50][0/5]\tBatch Time  2.535 ( 2.535)\tData Time  2.448 ( 2.448)\tTrain Loss 8.6675e+00 (8.6675e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:45:46,245][train_on_NDI.py][line:206][INFO] Epoch: [42/50], train loss 8.627950859069824, val loss 7.9061126708984375, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 12.5lr [0.0003561409668914375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [43/50][0/1]\tVal Loss 7.9061e+00 (7.9061e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30  12.50 ( 12.50)\n",
      "Training Progress\tEpoch: [44/50][0/5]\tBatch Time  2.236 ( 2.236)\tData Time  2.184 ( 2.184)\tTrain Loss 9.2956e+00 (9.2956e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:45:59,316][train_on_NDI.py][line:206][INFO] Epoch: [43/50], train loss 8.719072723388672, val loss 7.950137138366699, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 8.333333015441895lr [0.000285553045146602]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [44/50][0/1]\tVal Loss 7.9501e+00 (7.9501e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30   8.33 (  8.33)\n",
      "Training Progress\tEpoch: [45/50][0/5]\tBatch Time  2.236 ( 2.236)\tData Time  2.155 ( 2.155)\tTrain Loss 7.8498e+00 (7.8498e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:46:12,655][train_on_NDI.py][line:206][INFO] Epoch: [44/50], train loss 8.676364326477051, val loss 7.992510795593262, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 8.333333015441895lr [0.0002238031974265778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [45/50][0/1]\tVal Loss 7.9925e+00 (7.9925e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30   8.33 (  8.33)\n",
      "Training Progress\tEpoch: [46/50][0/5]\tBatch Time  1.725 ( 1.725)\tData Time  1.642 ( 1.642)\tTrain Loss 9.6189e+00 (9.6189e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:46:23,573][train_on_NDI.py][line:206][INFO] Epoch: [45/50], train loss 8.873651504516602, val loss 7.953723907470703, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 8.333333015441895lr [0.00017113512216949495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [46/50][0/1]\tVal Loss 7.9537e+00 (7.9537e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30   8.33 (  8.33)\n",
      "Training Progress\tEpoch: [47/50][0/5]\tBatch Time  2.700 ( 2.700)\tData Time  2.616 ( 2.616)\tTrain Loss 7.8313e+00 (7.8313e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:46:36,903][train_on_NDI.py][line:206][INFO] Epoch: [46/50], train loss 8.568894004821777, val loss 7.943626880645752, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 8.333333015441895lr [0.00012775667620663832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [47/50][0/1]\tVal Loss 7.9436e+00 (7.9436e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30   8.33 (  8.33)\n",
      "Training Progress\tEpoch: [48/50][0/5]\tBatch Time  2.635 ( 2.635)\tData Time  2.582 ( 2.582)\tTrain Loss 8.9465e+00 (8.9465e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:46:50,033][train_on_NDI.py][line:206][INFO] Epoch: [47/50], train loss 8.684251594543458, val loss 7.873816967010498, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 12.5lr [9.38390544464954e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [48/50][0/1]\tVal Loss 7.8738e+00 (7.8738e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30  12.50 ( 12.50)\n",
      "Training Progress\tEpoch: [49/50][0/5]\tBatch Time  2.112 ( 2.112)\tData Time  2.027 ( 2.027)\tTrain Loss 8.0097e+00 (8.0097e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:47:01,322][train_on_NDI.py][line:206][INFO] Epoch: [48/50], train loss 8.709074401855469, val loss 7.863563537597656, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 12.5lr [6.951611424666752e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [49/50][0/1]\tVal Loss 7.8636e+00 (7.8636e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30  12.50 ( 12.50)\n",
      "Training Progress\tEpoch: [50/50][0/5]\tBatch Time  1.139 ( 1.139)\tData Time  1.081 ( 1.081)\tTrain Loss 8.6024e+00 (8.6024e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-27 16:47:07,554][train_on_NDI.py][line:206][INFO] Epoch: [49/50], train loss 8.804769325256348, val loss 7.867000579833984, val acc @ 10 4.166666507720947, val acc @ 20 4.166666507720947, val acc @ 30 12.5lr [5.488384714002789e-05]\n",
      "[2023-04-27 16:47:07,555][train_on_NDI.py][line:230][INFO] Training Finished!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress\tEpoch: [50/50][0/1]\tVal Loss 7.8670e+00 (7.8670e+00)\tVal Acc@10   4.17 (  4.17)\tVal Acc@20   4.17 (  4.17)\tVal Acc@30  12.50 ( 12.50)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a95c1ff2904475d91fcdb70ee4cfd50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/train loss</td><td>█▆▆▇█▆▄▄▄▅▅▅▄▄▄▅▄▄▄▄▂▄▄▄▂▃▃▂▁▂▃▃▁▂▁▂▃▁▂▂</td></tr><tr><td>val/val acc @ 10</td><td>▆▁▆██▆▁▁▁▆▃▆▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>val/val acc @ 20</td><td>▅▁▇█▇▄▂▁▁▄▅▇▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>val/val acc @ 30</td><td>▅▃▆█▇▃▂▁▁▃█▇▅▃▄▄▄▅▄▄▄▃▃▃▄▄▃▄▃▃▃▃▄▃▄▃▃▃▄▄</td></tr><tr><td>val/val loss</td><td>▁▁▆▇▅▄▃▂▃▃▃▅▇▇████████████▇▇███▇▇▇▇███▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train/train loss</td><td>8.80477</td></tr><tr><td>val/val acc @ 10</td><td>4.16667</td></tr><tr><td>val/val acc @ 20</td><td>4.16667</td></tr><tr><td>val/val acc @ 30</td><td>12.5</td></tr><tr><td>val/val loss</td><td>7.867</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fold 0</strong> at: <a href='https://wandb.ai/zombiesama18/Lora%20test/runs/tghxeedt' target=\"_blank\">https://wandb.ai/zombiesama18/Lora%20test/runs/tghxeedt</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230427_163935-tghxeedt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/import/gp-home.cal/duanct/ProjectFolder/NDI_image_processing/wandb/run-20230427_164723-fc6pjwqe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zombiesama18/Lora%20test/runs/fc6pjwqe' target=\"_blank\">fold 1</a></strong> to <a href='https://wandb.ai/zombiesama18/Lora%20test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zombiesama18/Lora%20test' target=\"_blank\">https://wandb.ai/zombiesama18/Lora%20test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zombiesama18/Lora%20test/runs/fc6pjwqe' target=\"_blank\">https://wandb.ai/zombiesama18/Lora%20test/runs/fc6pjwqe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gp.sc.cc.tohoku.ac.jp/duanct/miniconda3/envs/py2cuda118/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'im_k'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 24\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[39m# scheduler = None\u001B[39;00m\n\u001B[1;32m     22\u001B[0m scheduler \u001B[39m=\u001B[39m torch\u001B[39m.\u001B[39moptim\u001B[39m.\u001B[39mlr_scheduler\u001B[39m.\u001B[39mCosineAnnealingLR(optimizer\u001B[39m=\u001B[39moptimizer, T_max\u001B[39m=\u001B[39mcfg\u001B[39m.\u001B[39mepochs, eta_min\u001B[39m=\u001B[39mcfg\u001B[39m.\u001B[39mbase_lr \u001B[39m*\u001B[39m \u001B[39m0.01\u001B[39m)\n\u001B[0;32m---> 24\u001B[0m train(cfg, logger, train_iter, val_iter, model, criterion, optimizer, cfg\u001B[39m.\u001B[39;49mepochs, scheduler\u001B[39m=\u001B[39;49mscheduler,\n\u001B[1;32m     25\u001B[0m         save_folder\u001B[39m=\u001B[39;49mcfg\u001B[39m.\u001B[39;49moutput_dir, wandb_config\u001B[39m=\u001B[39;49m\u001B[39mTrue\u001B[39;49;00m, device\u001B[39m=\u001B[39;49mdevice)\n\u001B[1;32m     26\u001B[0m wandb\u001B[39m.\u001B[39mfinish()\n",
      "File \u001B[0;32m/import/gp-home.cal/duanct/ProjectFolder/NDI_image_processing/train_on_NDI.py:199\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(args, logger, train_data, val_data, model, criterion, optimizer, total_epochs, save_folder, scheduler, wandb_config, device)\u001B[0m\n\u001B[1;32m    195\u001B[0m target_tensor \u001B[39m=\u001B[39m get_CNI_tensor(device, \u001B[39m200\u001B[39m)\n\u001B[1;32m    197\u001B[0m \u001B[39mfor\u001B[39;00m epoch \u001B[39min\u001B[39;00m \u001B[39mrange\u001B[39m(total_epochs):\n\u001B[1;32m    198\u001B[0m     train_loss, val_loss, val_acc_10, val_acc_20, val_acc_30 \u001B[39m=\u001B[39m \\\n\u001B[0;32m--> 199\u001B[0m         train_epoch(train_data, val_data, model, criterion, optimizer, epoch \u001B[39m+\u001B[39;49m \u001B[39m1\u001B[39;49m, total_epochs, target_tensor)\n\u001B[1;32m    201\u001B[0m     lr_info \u001B[39m=\u001B[39m \u001B[39m'\u001B[39m\u001B[39m'\u001B[39m\n\u001B[1;32m    202\u001B[0m     \u001B[39mif\u001B[39;00m scheduler:\n",
      "File \u001B[0;32m/import/gp-home.cal/duanct/ProjectFolder/NDI_image_processing/train_on_NDI.py:152\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[0;34m(train_data, val_data, model, criterion, optimizer, current_epoch, total_epoch, target_tensor)\u001B[0m\n\u001B[1;32m    150\u001B[0m data_time\u001B[39m.\u001B[39mupdate(time\u001B[39m.\u001B[39mtime() \u001B[39m-\u001B[39m start)\n\u001B[1;32m    151\u001B[0m origin, target, label \u001B[39m=\u001B[39m origin\u001B[39m.\u001B[39mcuda(), target\u001B[39m.\u001B[39mcuda(), label\u001B[39m.\u001B[39mcuda()\n\u001B[0;32m--> 152\u001B[0m em_ori, em_tar \u001B[39m=\u001B[39m model(origin, target)\n\u001B[1;32m    153\u001B[0m sim_mat \u001B[39m=\u001B[39m model\u001B[39m.\u001B[39mget_similarity_matrix(em_ori, em_tar)\n\u001B[1;32m    154\u001B[0m loss1 \u001B[39m=\u001B[39m criterion(sim_mat, torch\u001B[39m.\u001B[39marange(\u001B[39m0\u001B[39m, origin\u001B[39m.\u001B[39msize(\u001B[39m0\u001B[39m))\u001B[39m.\u001B[39mcuda())\n",
      "File \u001B[0;32m~/miniconda3/envs/py2cuda118/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[39m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_pre_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[39mor\u001B[39;00m _global_backward_pre_hooks \u001B[39mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[39mor\u001B[39;00m _global_forward_hooks \u001B[39mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[39mreturn\u001B[39;00m forward_call(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[39m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[39m=\u001B[39m [], []\n",
      "File \u001B[0;32m/import/gp-home.cal/duanct/ProjectFolder/NDI_image_processing/models/core_model.py:16\u001B[0m, in \u001B[0;36mRetrievalModel.forward\u001B[0;34m(self, im_q, im_k, **kwargs)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mforward\u001B[39m(\u001B[39mself\u001B[39m, im_q, im_k, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs):\n\u001B[0;32m---> 16\u001B[0m     q \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mencoder_q(im_q)\n\u001B[1;32m     17\u001B[0m     k \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mencoder_k(im_k)\n\u001B[1;32m     18\u001B[0m     \u001B[39mreturn\u001B[39;00m q, k\n",
      "File \u001B[0;32m~/miniconda3/envs/py2cuda118/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[39m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_pre_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[39mor\u001B[39;00m _global_backward_pre_hooks \u001B[39mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[39mor\u001B[39;00m _global_forward_hooks \u001B[39mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[39mreturn\u001B[39;00m forward_call(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[39m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[39m=\u001B[39m [], []\n",
      "\u001B[0;31mTypeError\u001B[0m: forward() missing 1 required positional argument: 'im_k'"
     ]
    }
   ],
   "source": [
    "logger_fname = datetime.datetime.now().strftime('%Y%m%d%H%M')\n",
    "logger = get_logger(f'./logs/{logger_fname}.log')\n",
    "\n",
    "logger.info(f'This training is to do {cfg.message_to_log}')\n",
    "\n",
    "for i, images in enumerate(k_fold_train_validation_split(ORIGINAL_IMAGE, TARGET_IMAGE, 7)):\n",
    "    wandb.init(project=cfg.message_to_log, group='lora_implementation', job_type='Imagenet_NDI_400E',\n",
    "                name=f'fold {i}', config=cfg.__dict__)\n",
    "    train_dataset = SingleChannelNDIDatasetContrastiveLearningWithAug(images, False, 200)\n",
    "    val_dataset = SingleChannelNDIDatasetContrastiveLearningWithAug(images, True, 200)\n",
    "    train_iter = DataLoader(train_dataset, cfg.batch_size, shuffle=True, drop_last=True)\n",
    "    val_iter = DataLoader(val_dataset, batch_size=len(val_dataset))\n",
    "\n",
    "    model = RetrievalModel(model)\n",
    "    model = model.cuda()\n",
    "\n",
    "    training_params = [param for param in model.parameters() if param.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params=training_params, lr=cfg.base_lr, weight_decay=cfg.weight_decay, momentum=cfg.momentum)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # scheduler = None\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=cfg.epochs, eta_min=cfg.base_lr * 0.01)\n",
    "\n",
    "    train(cfg, logger, train_iter, val_iter, model, criterion, optimizer, cfg.epochs, scheduler=scheduler,\n",
    "            save_folder=cfg.output_dir, wandb_config=True, device=device)\n",
    "    wandb.finish()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py2cuda118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
